[{"authors":["admin"],"categories":null,"content":"Who am I in this manipulated world?\nA lost sheep seeking for piousness.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://azraelzhor.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Who am I in this manipulated world?\nA lost sheep seeking for piousness.","tags":null,"title":"Thang Le","type":"authors"},{"authors":null,"categories":null,"content":"After discussing LDA in the previous part, we now get our hands dirty by implementing the Variational EM algorithm. Recall that the goal is to estimate $\\alpha, \\beta$ that maximizes the ELBO\n $$ \\mathrm{E}_{q}\\log p(w, \\theta, z;\\alpha, \\beta) - \\mathrm{E}_{q}\\log {q(z, \\theta;\\gamma, \\phi)} $$  where the joint likelihood and the variational distribution are factorized as follows\n$$ p(w, \\theta, z; \\alpha, \\beta) = Dir(\\theta;\\alpha)\\prod_{n=1}^{N}Cat(z_n;\\theta) Cat(w_n;z_n, \\beta) $$\n$$ q(z, \\theta; \\gamma, \\phi) = Dir(\\theta;\\gamma)\\prod_{n=1}^{N}Cat(z_n;\\phi_n) $$\nWorking with the ELBO But before getting into code, we need to derive the ELBO. Substituting these factorizations into the ELBO, we obtain\n \\begin{align} L \u0026 = \\mathrm{E}_{q}\\log p(w, \\theta, z;\\alpha, \\beta) - \\mathrm{E}_{q}\\log {q(z, \\theta;\\gamma, \\phi)} \\\\ \u0026 = \\mathrm{E}_{q}\\log Dir(\\theta; \\alpha) + \\sum_{n=1}^{N} \\Big[ \\mathrm{E}_{q} \\log Cat(z_n; \\theta) + \\mathrm{E}_{q} \\log Cat(w_n; z_n, \\beta) \\Big] \\\\ \u0026 \\quad - \\mathrm{E}_{q} \\log Dir(\\theta; \\gamma) - \\sum_{n=1}^{N}\\mathrm{E}_{q} \\log Cat (z_n;\\phi_n) \\tag{1} \\end{align}  Dealing with expected values To handle the expectations in the ELBO, we need to rewrite the Dirichlet distribution in exponential form as follows\n \\begin{align} Dir(x;\\alpha) \u0026 = \\frac{1}{B(\\alpha)} \\prod_{k=1}^{K}x_k^{\\alpha_k - 1} \\\\ \u0026 = \\frac{\\Gamma(\\sum_{k=1}^{K}\\alpha_k)}{\\sum_{k=1}^{K}\\Gamma(\\alpha_k)} \\prod_{k=1}^{K}x_k^{\\alpha_k - 1} \\\\ \u0026 = \\exp\\Big[ \\sum_{k=1}^{K} (\\alpha_k - 1) \\log x_k + \\log \\Gamma(\\sum_{k=1}^{K}\\alpha_k) - \\sum_{k=1}^{K}\\log\\Gamma(\\alpha_k) \\Big] \\end{align}   Exponential family distribution $$ p(x|\\theta) = h(x) exp(\\eta \\cdot T(x) - A(\\eta)) $$\nwhere $h(x)$ is known as base measure, $\\eta(\\theta)$ is natural parameter, $T(x)$ is sufficient statistic and $A(\\theta)$ is log normalizer. One important property of the exponential family is that the mean of the sufficient statistic $T(x)$ can be derived by differentiating the natural parameter $A(\\eta)$ $$ E[T_j]= \\frac{\\partial A(\\eta)}{\\partial \\eta_j} \\tag{2} $$\n Applying property $(2)$ for the case of the Dirichlet distribution, we have\n \\begin{align} \\mathrm{E}_{x \\sim Dir(x;\\alpha)}\\log x_k \u0026 = \\frac{\\partial(\\sum_{j=1}^{K}\\log\\Gamma(\\alpha_j) - \\log \\Gamma(\\sum_{j=1}^{K}\\alpha_j)) }{\\partial (\\alpha_k - 1)} \\\\ \u0026 = \\frac{\\partial(\\sum_{j=1}^{K}\\log\\Gamma(\\alpha_j) - \\log \\Gamma(\\sum_{j=1}^{K}\\alpha_j)) }{\\partial \\alpha_k} \\\\ \u0026 = \\Psi(\\alpha_k) - \\Psi(\\sum_{j=1}^{K}\\alpha_j) \\end{align}  where $\\Psi(\\cdot)$ is the derivative of the logarithm of Gamma function (also known as Digamma).\nAlso, the categorical distribution can be represented using Iverson bracket $[\\cdot]$\n$$ Cat(x;\\theta) = \\prod_{i=1}^K \\theta_i^{[x=i]} $$\nwhere $[x=i]$ evaluates to $1$ if $x = i$, $0$ otherwise (with the assumption that values of $x$ fall into the range ${1, 2, \u0026hellip;, K}$).\nExpectation of a function $f(x)$ with respect to the categorical distribution is computed as\n $$ \\mathrm{E}_{x\\sim Cat(x;\\theta)} f(x) = \\sum_{i=1}^{K} \\theta_i f(i) $$  Deriving the ELBO Using these results above, we have\n \\begin{align} \u0026 \\mathrm{E}_{q}\\log Dir(\\theta; \\alpha) \\\\ \u0026 = \\mathrm{E}_{q} \\Big[\\sum_{k=1}^{K} (\\alpha_k - 1) \\log \\theta_k + \\log \\Gamma(\\sum_{k=1}^{K}\\alpha_k) - \\sum_{k=1}^{K}\\log\\Gamma(\\alpha_k)\\Big] \\\\ \u0026 = \\sum_{k=1}^{K}(\\alpha_k - 1) \\mathrm{E}_{\\theta \\sim Dir(\\theta;\\gamma)} \\log\\theta_k + \\log \\Gamma(\\sum_{k=1}^{K}\\alpha_k) - \\sum_{k=1}^{K}\\log\\Gamma(\\alpha_k) \\\\ \u0026 = \\sum_{k=1}^{K}(\\alpha_k - 1) (\\Psi(\\gamma_k) - \\Psi(\\sum_{j=1}^{K}\\gamma_j)) + \\log \\Gamma(\\sum_{k=1}^{K}\\alpha_k) - \\sum_{k=1}^{K}\\log\\Gamma(\\alpha_k) \\tag{3} \\\\ \u0026\\\\ \u0026 \\mathrm{E}_{q} \\log Cat(z_n; \\theta) \\\\ \u0026 = E_{z_n \\sim Cat(z_n;\\phi_n), \\theta \\sim Dir(\\theta;\\gamma)} \\log Cat(z_n;\\theta) \\\\ \u0026 = E_{z_n \\sim Cat(z_n;\\phi_n), \\theta \\sim Dir(\\theta;\\gamma)} \\sum_{j=1}^{K}[z_n=j]\\log \\theta_j \\\\ \u0026 = \\sum_{i=1}^{K} \\phi_{ni} \\mathrm{E}_{\\theta \\sim Dir(\\theta; \\gamma)} \\sum_{j=1}^{K}[i=j]\\log \\theta_j \\\\ \u0026 = \\sum_{i=1}^{K} \\phi_{ni} \\mathrm{E}_{\\theta \\sim Dir(\\theta; \\gamma)} \\log \\theta_i \\\\ \u0026 = \\sum_{i=1}^{K} \\phi_{ni}(\\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^{K}\\gamma_j)) \\tag{4} \\\\ \u0026\\\\ \u0026 \\mathrm{E}_{q} \\log Cat(w_n;z_n, \\beta) \\\\ \u0026 = \\mathrm{E}_{z_n \\sim Cat(z_n;\\phi_n)} \\log Cat(w_n;\\beta_{z_n}) \\\\ \u0026 = \\mathrm{E}_{z_n \\sim Cat(z_n;\\phi_n)} \\sum_{j=1}^{V} [w_n=j] \\log \\beta_{z_n j} \\\\ \u0026\\quad \\textrm{(assumming that $w_n$ represents the index of word in the vocabulary)} \\\\ \u0026 = \\sum_{i=1}^{K} \\phi_{ni} \\sum_{j=1}^{V} [w_n=j] \\log \\beta_{ij}\\\\ \u0026 = \\sum_{i=1}^{K} \\sum_{j=1}^{V} \\phi_{ni} [w_n=j] \\log \\beta_{ij} \\tag{5} \\\\ \u0026\\\\ \u0026 \\mathrm{E}_{q} \\log Dir(\\theta;\\gamma) \\\\ \u0026 = \\mathrm{E}_{q} \\Big[\\sum_{k=1}^{K} (\\gamma_k - 1) \\log \\theta_k + \\log \\Gamma(\\sum_{k=1}^{K}\\gamma_k) - \\sum_{k=1}^{K}\\log\\Gamma(\\gamma_k)\\Big] \\\\ \u0026 = \\sum_{k=1}^{K}(\\gamma_k - 1) \\mathrm{E}_{\\theta \\sim Dir(\\theta;\\gamma)} \\log\\theta_k + \\log \\Gamma(\\sum_{k=1}^{K}\\gamma_k) - \\sum_{k=1}^{K}\\log\\Gamma(\\alpha_k) \\\\ \u0026 = \\sum_{k=1}^{K}(\\gamma_k - 1) (\\Psi(\\gamma_k) - \\Psi(\\sum_{j=1}^{K}\\gamma_j)) + \\log \\Gamma(\\sum_{k=1}^{K}\\gamma_k) - \\sum_{k=1}^{K}\\log\\Gamma(\\gamma_k) \\tag{6} \\\\ \u0026\\\\ \u0026 \\mathrm{E}_{q} \\log q(z_n; \\phi_n) \\\\ \u0026 = \\mathrm{E}_{z_n \\sim Cat(z_n;\\phi_n)} \\log Cat(z_n;\\phi_n) \\\\ \u0026 = \\mathrm{E}_{z_n \\sim Cat(z_n;\\phi_n)} \\sum_{j=1}^{K}[z_n = j]\\log \\phi_{nj} \\\\ \u0026 = \\sum_{i=1}^{K} \\phi_{ni} \\sum_{j=1}^{K} [i=j] \\log \\phi_{nj} \\\\ \u0026 = \\sum_{i=1}^{K} \\phi_{ni} \\log \\phi_{ni} \\tag{7} \\end{align}  Substituting $(3), (4), (5), (6), (7)$ into $(1)$, the ELBO becomes\n \\begin{align} L\u0026(\\gamma, \\phi;\\alpha, \\beta) \\\\ = \u0026\\sum_{k=1}^{K}(\\alpha_k - 1) (\\Psi(\\gamma_k) - \\Psi(\\sum_{j=1}^{K}\\gamma_j)) + \\log \\Gamma(\\sum_{k=1}^{K}\\alpha_k) - \\sum_{k=1}^{K}\\log\\Gamma(\\alpha_k) \\\\ \u0026 + \\sum_{n=1}^{N} \\sum_{i=1}^{K} \\phi_{ni}(\\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^{K}\\gamma_j)) \\\\ \u0026 + \\sum_{n=1}^{N} \\sum_{i=1}^{K} \\sum_{j=1}^{V} \\phi_{ni} [w_n = j]\\log \\beta_{ij}\\\\ \u0026 - \\sum_{k=1}^{K}(\\gamma_k - 1) (\\Psi(\\gamma_k) - \\Psi(\\sum_{j=1}^{K}\\gamma_j)) - \\log \\Gamma(\\sum_{k=1}^{K}\\gamma_k) + \\sum_{k=1}^{K}\\log\\Gamma(\\gamma_k) \\\\ \u0026 - \\sum_{n=1}^{N} \\sum_{i=1}^{K} \\phi_{ni} \\log \\phi_{ni} \\tag{8} \\end{align}  which is now much easier to deal with. Preparing data Now we dive into the code. For illustration purpose, we use a public dataset from Kaggle. The dataset contains news headlines crawled from ABC News. Here is some code to load the data\nnews_data_path = \u0026quot;abcnews-date-text.csv\u0026quot; gdown.download(\u0026quot;https://drive.google.com/uc?id=1BGaMi0XURByE0WM4omDwskoq83WnTXyx\u0026quot;, news_data_path, quiet=False) data_df = pd.read_csv(news_data_path, error_bad_lines=False); data_df.head()  A small piece of the data will look like this\n .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } td, th, tr { border: 1px solid #ddd; }  \n  publish_date headline_text     0 20030219 aba decides against community broadcasting lic...   1 20030219 act fire witnesses must be aware of defamation   2 20030219 a g calls for infrastructure protection summit   3 20030219 air nz staff in aust strike for pay rise   4 20030219 air nz strike to affect australian travellers     Preprocessing data There is a total of $1186018$ headlines in the original dataset but for a quick experiment, we extract the very first $10000$ headlines only\ndata = data_df[\u0026quot;headline_text\u0026quot;][:10000]  Then, we need to do some preprocessing stuff\n Remove the stop words using stopwords from nltk package Build the vocabulary with word2idx and idx2word Create the corpus containing all documents  corpus = [] word2idx = {} idx2word = {} for line in data: doc = [w for w in line.split(' ') if w not in stopwords.words()] for word in doc: if word not in word2idx: word2idx[word] = len(word2idx) idx2word[len(idx2word)] = word corpus.append(doc)  Global configuration Next, we set up some global configuration before implementing LDA model.\nmax_doc_length = 0 for doc in corpus: if max_doc_length \u0026lt; len(doc): max_doc_length = len(doc) class Config: corpus = corpus word2idx = word2idx idx2word = idx2word num_vocabs = len(word2idx) # V max_doc_length = max_doc_length # N va_threshold = 1e-6 # threshold for variational infrence em_threshold = 1e-4 # threshold for variational EM  LDA model definition We then define an LDA class to handle the main logic of Variational EM\nclass LDA(object): def __init__(self, corpus, num_topics, num_words, num_vocabs, word2idx, idx2word): self.corpus = corpus # collection of documents self.K = num_topics # number of topics in total self.V = num_vocabs # number of vocabulary self.word2idx = word2idx self.idx2word = idx2word # model parameters self.alpha = None self.beta = None # sufficient statistics self.beta_ss = None  Compute the log-likelihood Evaluating $(8)$ requires the computation of Gamma and Digamma functions. Fortunately, we can make use of the scipy package to handle the heavy work.\nfrom scipy.special import digamma, loggamma  We can then implement the log_likelihood function for current document and variational parameters $\\gamma$, $\\phi$; given model parameters $\\alpha, \\beta$\ndef log_likelihood(doc, gamma, phi): \u0026quot;\u0026quot;\u0026quot; Compute the (approximate) log-likelihood \u0026quot;\u0026quot;\u0026quot; # (K,) digamma_derivative = digamma(gamma) - digamma(np.sum(gamma)) l1 = loggamma(np.sum(self.alpha)) - np.sum(loggamma(self.alpha)) \\ + np.sum((self.alpha - 1) * digamma_derivative) \\ - loggamma(np.sum(gamma)) + np.sum(loggamma(gamma)) \\ - np.sum((gamma - 1) * digamma_derivative) l2 = 0 for i in range(self.K): for n in range(len(doc)): if phi[n, i] \u0026gt; 0: l2 += phi[n, i] * (digamma_derivative[i] \\ + np.log(self.beta[i, self.word2idx[doc[n]]]) \\ - np.log(phi[n, i])) return l1 + l2  Variational Inference The goal of variational inference is to find the optimal $\\phi^*, \\gamma^*$ for the mean-field distribution $q(z, \\theta; \\gamma, \\phi)$. By taking derivatives of $(8)$ with respect to $\\phi, \\gamma$ and set it to zero, we obtain coordinate updates $$ \\phi_{ni} \\propto \\beta_{iv} \\exp(\\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^{K}\\gamma_j)) $$\n$$ \\gamma_i = \\alpha_i + \\sum_{n=1}^{N} \\phi_{ni} $$\nwhere $v$ denotes the unique index of the word $w_n$ in the vocabulary. For more detailed derivation of these updates, we refer to Appendix A (section 3.1, 3.2) of the original LDA paper. Then, we can implement the coordinate ascent algorithm for variational inference as follows\ndef variational_inference(self, doc): \u0026quot;\u0026quot;\u0026quot; Do the variational inference for each document \u0026quot;\u0026quot;\u0026quot; N = len(doc) # init variational parameters # (N, K) phi = np.full(N, self.K), 1.0 / self.K) # (K,) gamma = self.alpha + N * 1.0 / self.K old_likelihood = -math.inf # coordinate ascent while True: # update phi for n in range(N): for i in range(self.K): phi[n, i] = self.beta[i, self.word2idx[doc[n]]] \\ * np.exp(digamma(gamma[i])) # normalize phi phi = phi / np.sum(phi, axis=1, keepdims=True) # update gamma gamma = self.alpha + np.sum(phi, axis=0) likelihood = self.log_likelihood(doc, gamma, phi) converged = (old_likelihood - likelihood) / likelihood old_likelihood = likelihood if converged \u0026lt; cfg.va_threshold: break return phi, gamma, likelihood  Variational EM Recall that the Variational EM algorithm consisting of\n Initialize parameters $\\alpha^{(0)}, \\beta^{(0)}$ For each loop $t$ start from $0$  E step: For each document $d$, obtain the optimal $\\gamma^{(d)}, \\phi^{(d)}$ of the variational distribution $q(z, \\theta; \\gamma, \\phi) = q(\\theta;\\gamma)\\prod_{n=1}^{N}q(z_n;\\phi_n)$ M step: Maximize the expected log-likelihood (up to some constant) $$ \\mathop{max}_{{\\alpha^{(t+1)}, \\beta^{(t+1)}}} \\sum_{d=1}^{M} \\mathrm{E}_{z, \\theta \\sim q(z, \\theta; \\gamma^{(d)}, \\phi^{(d)})} {\\log p(w, z, \\theta ;{{\\alpha^{(t+1)}, \\beta^{(t+1)}}}}) $$  If the convergence standard is satisfied, stop    Hence, the Variational EM algorithm can be implemented as function variational_em below\ndef variational_em(self): \u0026quot;\u0026quot;\u0026quot; Fit LDA model using variational EM \u0026quot;\u0026quot;\u0026quot; self.init_param() old_llhood = -math.inf ite = 0 while True: ite += 1 llhood = self.variational_e_step(self.corpus) self.m_step() converged = (old_llhood - llhood) / llhood old_llhood = llhood print(\u0026quot;STEP EM: {} - Likelihood: {} - Converged rate: {}\u0026quot;.\\ format(ite, llhood, converged)) if converged \u0026lt; cfg.em_threshold: break  The function init_param is to initialize parameter $\\alpha, \\beta$\ndef init_param(self): \u0026quot;\u0026quot;\u0026quot; Init parameters \u0026quot;\u0026quot;\u0026quot; self.alpha = np.full(self.K, 1.0) self.beta = np.random.randint(1, 50, (self.K, self.V)) self.beta = self.beta / np.sum(self.beta, axis=1, keepdims=True)  Then, two functions variational_e_step and m_step are corresponding to E-step and M-step of the algorithm, respectively.\nE Step In the E-step, we perform variational inference for each document $d$ to obtain $\\phi^{(d)}, \\gamma^{(d)}$\ndef variational_e_step(self, corpus): \u0026quot;\u0026quot;\u0026quot; Approximate the posterior distribution : corpus - list of documents \u0026quot;\u0026quot;\u0026quot; total_likelihood = 0 self.beta_ss = np.zeros((self.K, self.V)) + 1e-20 for i, doc in enumerate(corpus): phi, gamma, doc_likelihood = self.variational_inference(doc) # add to total likelihood total_likelihood += doc_likelihood # update statistics for n in range(len(doc)): for k in range(self.K): self.beta_ss[k, self.word2idx[doc[n]]] += phi[n, k] return total_likelihood  M Step In M-step, we obtain optimal $\\alpha, \\beta$. Though, the optimal update for $\\alpha$ is kind of complex (Appendix A, section 4.2). Thus, for a simple illustration, we consider alpha as fixed in the scope of this blog. Setting the derivate of the ELBO $(8)$ with respect to $\\beta$ to zero, we yield\n$$ \\beta_{ij} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N} \\phi_{ni}^{(d)} w_{n}^{j} \\quad \\textrm{(Appendix A, section 4.1)} $$\nFor coding convenience, we implement these updates right in the function variational_e_step and store these unnormalized results in the variable self.beta_ss. Hence, in the M-step, we just normalize and assign it to self.beta\ndef m_step(self): \u0026quot;\u0026quot;\u0026quot; Maximum likelihood estimation \u0026quot;\u0026quot;\u0026quot; # alpha is considered fixed known constant, hence skip here # self.alpha # (K, V) self.beta = self.beta_ss / np.sum(self.beta_ss, axis=1, keepdims=True)  Training Now everything is setup. We then run the following code for training LDA\nmodel = LDA(corpus=cfg.corpus, num_topics=10, num_words=cfg.max_doc_length, num_vocabs=cfg.num_vocabs, word2idx=cfg.word2idx, idx2word=cfg.idx2word) model.variational_em()  The output will be like\nSTEP EM: 1 - Likelihood: -357373.4968604174 - Converged rate: 1.798192951590305 STEP EM: 2 - Likelihood: -303489.3001228882 - Converged rate: 0.1775489175918577 STEP EM: 3 - Likelihood: -299860.97107707005 - Converged rate: 0.012100037670076039 STEP EM: 4 - Likelihood: -294189.9329730006 - Converged rate: 0.019276791856062188 STEP EM: 5 - Likelihood: -287787.73198189674 - Converged rate: 0.02224626097510863 STEP EM: 6 - Likelihood: -282157.87102739484 - Converged rate: 0.0199528757925569 STEP EM: 7 - Likelihood: -277893.16594093136 - Converged rate: 0.015346563388931922 ........................................... STEP EM: 32 - Likelihood: -264753.70609608945 - Converged rate: 0.00017462683995490507 STEP EM: 33 - Likelihood: -264713.55673188687 - Converged rate: 0.00015167097861647994 STEP EM: 34 - Likelihood: -264676.9793795508 - Converged rate: 0.0001381961983313688 STEP EM: 35 - Likelihood: -264643.03067223117 - Converged rate: 0.00012828113112741676 STEP EM: 36 - Likelihood: -264611.45086573914 - Converged rate: 0.000119344066134337 STEP EM: 37 - Likelihood: -264582.51780543657 - Converged rate: 0.0001093536358432039 STEP EM: 38 - Likelihood: -264556.4361364331 - Converged rate: 9.85864089506385e-05  Result After training, we can extract the top words of the 10 \u0026ldquo;abstract\u0026rdquo; topics.\ntopk = 10 indices = np.argpartition(model.beta, -topk, axis=1)[:, -topk:] topic_top_words_dict = {} for k in range(model.K): topic_top_words_dict[\u0026quot;Topic {}\u0026quot;.format(k + 1)] = \\ [model.idx2word[idx] for idx in indices[k]] topic_df = pd.DataFrame(topic_top_words_dict) topic_df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9 Topic 10     0 air murder gets continue denies tas community high act lead   1 takes probe season protesters farmers south public ban three australian   2 baghdad charged trial water vic final go work forces union   3 election aust saddam found korea clash water continues claims four   4 first pm home back boost get missing wins dead minister   5 coast crash top plan urged world new set council mp   6 oil may win anti govt call killed calls support qld   7 coalition iraqi still death iraq cup fire funds us british   8 howard court two sars north australia rain security troops wa   9 nsw police woman protest health report hospital drought says group     Full code available here.\nReferences\n Latent Dirichlet Allocation. David M. Blei, Andrew Y. Ng, Michael I. Jordan. 2003 ( pdf) Dataset ( link)  ","date":1589241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589241600,"objectID":"4da41e2dc54daefd2e4dac71fdd3b308","permalink":"https://azraelzhor.github.io/post/lda-part-2/","publishdate":"2020-05-12T00:00:00Z","relpermalink":"/post/lda-part-2/","section":"post","summary":"After discussing LDA in the previous part, we now get our hands dirty by implementing the Variational EM algorithm. Recall that the goal is to estimate $\\alpha, \\beta$ that maximizes the ELBO\n $$ \\mathrm{E}_{q}\\log p(w, \\theta, z;\\alpha, \\beta) - \\mathrm{E}_{q}\\log {q(z, \\theta;\\gamma, \\phi)} $$  where the joint likelihood and the variational distribution are factorized as follows\n$$ p(w, \\theta, z; \\alpha, \\beta) = Dir(\\theta;\\alpha)\\prod_{n=1}^{N}Cat(z_n;\\theta) Cat(w_n;z_n, \\beta) $$\n$$ q(z, \\theta; \\gamma, \\phi) = Dir(\\theta;\\gamma)\\prod_{n=1}^{N}Cat(z_n;\\phi_n) $$","tags":["latent dirichlet allocation","lda","topic modeling","python","variational inference","variational em"],"title":"Variational Expectation Maximization for Latent Dirichlet Allocation - Part 2","type":"post"},{"authors":[],"categories":[],"content":"","date":1585909556,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585909556,"objectID":"aaa8d84e4c82a866439d720501f86fd3","permalink":"https://azraelzhor.github.io/project/vietnamese-data-augmentation/","publishdate":"2020-04-03T17:25:56+07:00","relpermalink":"/project/vietnamese-data-augmentation/","section":"project","summary":"","tags":[],"title":"Vietnamese Data Augmentation","type":"project"},{"authors":[],"categories":[],"content":"","date":1585909502,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585909502,"objectID":"86005a2c5cda51f0f777c71d91ced75d","permalink":"https://azraelzhor.github.io/project/translating/","publishdate":"2020-04-03T17:25:02+07:00","relpermalink":"/project/translating/","section":"project","summary":"","tags":[],"title":"Translating","type":"project"},{"authors":[],"categories":null,"content":"","date":1585906950,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585906950,"objectID":"39cbbac7bfcc4ecc53ed66d8b301d8f7","permalink":"https://azraelzhor.github.io/talk/latent-dirichlet-allocation/","publishdate":"2020-04-03T16:42:30+07:00","relpermalink":"/talk/latent-dirichlet-allocation/","section":"talk","summary":"","tags":["lda","topic modeling"],"title":"Latent Dirichlet Allocation","type":"talk"},{"authors":null,"categories":null,"content":"Text data is everywhere. When having massive amounts of them, a need naturally arises is that we want them to be organized efficiently. A naive way is to organize them based on topics, meaning that text covering the same topics should be put into the same groups. The problem is that we do not know which topics a text document belongs to and manually labeling topics for all of them is very expensive. Hence, topic modeling comes as an efficient way to automatically discover abstract topics contained in these text documents.\nOne of the most common topic models is Latent Dirichlet Allocation (LDA), was introduced long time ago (D. Blei et al, 2003) but is still powerful now. LDA is a complex, hierarchical latent variable model with some probabilistic assumptions over it. Thus, before diving into detail of LDA, let us review some knowledges about latent variable model and how to handle some problems associated with it.\n Note: My blog on LDA contains two parts. This is the first part about theoretical understanding of LDA. The second part involves a basic implementation of LDA, which you can check out here.\n Latent variable model A latent variable model assumes that data, which we can observe, is controlled by some underlying unknown factor. This dependency is often parameterized by a known distribution along with its associated parameters, known as model parameter. A simple latent variable model consists of three parts: observed data $x$, latent variable $z$ that controls $x$ and model parameter $\\theta$ like the picture below. Latent variables increases our model\u0026rsquo;s expressiveness (meaning our model can capture more complex data) but there\u0026rsquo;s no such thing as a free lunch. Typically, there are two main problems associated with latent variable models that need to be solved\n The first one is learning in which we try to find the \u0026ldquo;optimal\u0026rdquo; parameters ${\\theta^*}$ based on some criterion. One powerful technique for learning is maximum likelihood estimation preferring to chose the parameters that maximize the likelihood $p(x;\\theta)$. Though, maximum likelihood estimation in latent variable models is hard. A maximum likelihood method named Expectation Maximization can solve this difficulty for some kind of models. It is also helpful for LDA so we will discuss the method in the next section.    In many cases, latent variables can capture meaningful pattern in the data. Hence, given new data, we are often interested in the value of latent variables. This raises the problem of inference where we want to deduce the posterior $p(x|z;\\theta)$. Though, in many cases the posterior is hard to compute. For example, when $z$ is continuous, we have $$ p(z|x;\\theta) = \\frac{p(x, z ;\\theta)}{p(x;\\theta)} = \\frac{p(x, z ;\\theta)}{\\int_z p(x, z;\\theta)} $$\nThe integral in the denominator often makes the posterior intractable. A method to solve this problem, named Variational Inference, will be discussed later.\n  Expectation Maximization (EM) Introducing latent variables to a statistical model makes its likelihood function non-convex. Thus, it becomes hard to find a maximum likelihood solution. The EM algorithm was introduced to solve the maximum likelihood estimation problem in these kind of statistical models. The algorithm iteratively alternates between building an expected log-likelihood (E step), which is a convex lower bound to the non-convex log-likelihood, and maximizing it over parameters (M step).\nBut how does EM construct the expected log-likelihood? We have\n \\begin{align} \\log p(x; \\theta) \u0026 \\geq \\log p(x;\\theta) - KL({{q(z)}}||p(z|x;\\theta)) \\\\ \u0026 = \\log p(x;\\theta) - (\\mathrm{E}_{z\\sim q(z)}\\log q(z) - \\mathrm{E}_{z \\sim q(z)}\\log p(z|x; \\theta)) \\\\ \u0026 = \\mathrm{E}_{z\\sim {q(z)}}(\\log p(x;\\theta) + \\log p(z|x;\\theta)) - \\mathrm{E}_{z\\sim q{(z)}}\\log {q(z)} \\\\ \u0026 = \\mathrm{E}_{z\\sim q(z)}\\log p(x, z;\\theta) - \\mathrm{E}_{z\\sim q(z)}\\log {q(z)} = L(q, \\theta) \\tag{1} \\\\ \\end{align}  for any choice of $q(z)$. It is obvious that $L(q, \\theta)$ is a lower bound of $\\log p(x;\\theta)$ and the equality holds if and only if $q(z) = p(z|x;\\theta)$. EM aims to construct a lower bound that is easy to maximize. By initializing parameter $\\theta_{old}$ and choosing $q(z) = p(z|x;\\theta_{old})$ at each E-step, the lower bound becomes\n $$ L(\\theta) = \\mathrm{E}_{z\\sim p(z|x;\\theta_{old})} \\log p(x,z;\\theta) - \\mathrm{E}_{z\\sim {p(z|x;\\theta_{old})}}\\log {p(z|x;\\theta_{old})} $$  EM then maximizes $L(\\theta)$ at each M-step\n \\begin{align} \\mathop{max}_{\\theta} L(\\theta) \u0026 = \\mathop{max}_{\\theta} \\mathrm{E}_{z\\sim p(z|x;\\theta_{old})} \\log p(x,z;\\theta) - \\mathrm{E}_{z\\sim {p(z|x;\\theta_{old})}}\\log {p(z|x;\\theta_{old})} \\\\ \u0026 = \\mathop{max}_{\\theta} \\mathrm{E}_{z\\sim p(z|x;\\theta_{old})} \\log p(x,z;\\theta) \\\\ \\end{align}  The EM algorithm can be summarized as follows\n Initialize parameter $\\theta = \\theta^{(0)}$ For each loop $t$ start from $0$  Estimate the posterior $p(z|x; {\\theta^{(t)}})$ Maximize the expected log-likelihood $\\mathop{max}_{{\\theta^{(t+1)}}} \\mathrm{E}_{z\\sim p(z|x ;{\\theta^{(t)}})} {p(x, z ;{{\\theta^{(t+1)}}}})$  If the convergence standard is satisfied, stop     Note: It is easy to notice that the EM algorithm can only be applied if we can compute (or approximate) the posterior distribution analytically, given the current parameter ${\\theta^{(t)}}$.\n If you want to go into the details of EM, Gaussian Mixture (when $z$ is discrete) and Probabilistic Principal Component Analysis (or Probabilistic PCA in short, when $z$ is continuous) are the two perfect cases to study.\nVariational Inference In many of the cases, the posterior distribution $p(z|x;\\theta)$ we are interested in can not be inferred analytically, or in other words, it is intractable. This leads naturally to the field of approximate inference, in which we try to approximate the intractable posterior. Variational inference is such a technique in approximate inference which is fast and effective enough for a good approximation of $p(z|x;\\theta)$. The process can be pictured as follows As we can see, the idea of variational inference is simple that we reformulate the problem of inference as an optimization problem by\n First, posit a variational family ${q(z;v)}$ controlled by variational parameter $v$ Then, find the optimal ${q(z;v^*)}$ in this family, which is as \u0026ldquo;close\u0026rdquo; to $p(z|x;\\theta)$ as possible  Specifically, the goal of variational inference is then to minimize the $KL$ divergence between the variational family and the true posterior: $\\mathop{min}_{q, v}KL({{q(z;v)}}||p(z|x;\\theta))$. But how can we minimize such an intractable term?\nRecall from $(1)$ (with the variational distribution ${q(z;v)}$ now being chosen as ${q(z)}$), we have the $ELBO$\n \\begin{align} \u0026 \\log p(x;\\theta) - KL({{q(z;v)}}||p(z|x;\\theta))\\\\ \u0026 = \\mathrm{E}_{z\\sim {q(z;v)}}\\log p(x, z;\\theta) - \\mathrm{E}_{z\\sim {q(z;v)}}\\log {q(z;v)}\\\\ \\end{align}  Since $\\log p(x;\\theta)$ is considered as constant, minimizing the KL divergence is equivalent to maximizing the $ELBO$. The optimization problem becomes\n $$ \\mathop{max}_{q}\\mathrm{E}_{z\\sim {q(z;v)}}\\log p(x, z;\\theta) - \\mathrm{E}_{z\\sim {q(z;v)}}\\log {{q(z;v)}} \\tag{2} $$  which now can be optimized with a suitable choice of ${q(z;v)}$.\n Note: EM and variational inference both involve maximizing the ELBO of the log-likelihood. However, EM produces a point estimate of the optimal model parameter, meanwhile variational inference results in an approximation of the posterior distribution.\n Mean field approximation Several forms of variational inference has been proposed to design a tractable variational family. The simplest out of them is mean-field approximation, which makes a strong assumption that all latent variables are mutually independent. The variational distribution can then be factorized as $$ q(z;v) = \\prod_{k=1}^{K}q(z_k;v_k) \\tag{3} $$\nwhere $z$ consists of $K$ latent variables $(z_1, z_2, \u0026hellip;, z_K)$ and each latent variable $z_k$ is controlled by its own variational parameter $v_k$.\nWe will not go into detail here, but substituting $(3)$ into $(2)$, taking the derivative with respect to each $q(z_k;v_k)$, then setting the derivative to zero we obtain the coordinate ascent update\n$$ q^*(z_k;v_k^*) \\propto \\mathrm{E}_{z_{-k} \\sim q_{-k}(z_{-k};{v_{-k}})} \\log p(z_k, z_{-k}, x;\\theta) \\tag{4} $$\nwhere $(\\cdot)_{-k}$ denotes all but the $k$th element.\nNote that until now we didn\u0026rsquo;t specify the functional form for each variational factor $q(z_k;v_k)$ yet. Fortunately, the optimal form of each $q(z_k;v_k)$ can be derived from the $RHS$ expression of the coordinate update $(4)$, which is often easy to work with for many models.\nCoordinate ascent update We can then use the coordinate ascent algorithm to find the optimal mean-field distribution. The algorithm can be summarized as follows\n Initialize $v = v^{(0)}$ For each loop $t$ start from $0$  For each loop $k$ from $1$ to $K$  Estimate $q^*(z_k;v_k^*) \\propto \\mathrm{E}_{z_{-k} \\sim q_{-k}(z_{-k};{v_{-k}}^{(t)})} \\log p(z_k, z_{-k}, x;\\theta)$ Set $q(z_k; v_k^{(t+1)}) = q^*(z_k;v_k^*)$   Compute the $ELBO$ to check convergence    By now we are familiar with the concept of latent variable model. Let us move on to discuss LDA in the next section.\nLatent Dirichlet Allocation LDA is a latent variable model on observed text data or, to be more specific, a collection of words in each document. The model is built based on the assumptions that each document is a distribution over a predefined number of topics; meanwhile, each topic is considered as a distribution over words in a fixed vocabulary. For example, suppose that we have $4$ topics \u0026lt;economics, animal, science, art\u0026gt; and a total of $6$ words \u0026lt;money, astronomy, investment, rabbit, painting, chemical\u0026gt;. Then our assumptions can be illustrated like this figure below.\nThe two probabilistic assumptions of LDA\nAlso, according to this figure, we can say that the document is $40%$ of economics, $20%$ of animal, $30%$ of science and $10%$ of art. Seem familiar? This is basically the categorical distribution.\nCategorical distribution Formally, categorical distribution is a discrete probability distribution, describing the possibility that one discrete random variable belongs to one of $K$ categories. The distribution is parameterized by a $K$-dimensional vector $\\theta$ denoting probabilities assigned to each category. It probability mass function is defined as $$ p(x=i) = \\theta_i $$\nwhere $x$ is the random variable and $i$ ranges from $1$ to $K$ (representing the $K$ categories).\nIn our example above, the document is a categorical distribution over $K = 4$ topics, with its parameter $\\theta = [0.4, 0.2, 0.3, 0.1]$. Similarly, each topic is also a categorical distribution over $K = 6$ words.\nDirichlet distribution Another distribution which plays an important role in LDA is the Dirichlet distribution (hence the name LDA). Dirichlet distribution is a continuous multivariate probability distribution over a $(K-1)$-simplex, which can be seen as a set of $K$-dimensional vectors $x=[x_1, x_2, \u0026hellip;, x_K]$ such that each $x_k \\geq 0$ and $\\sum_{k=1}^Kx_k = 1$. For example, the 2-simplex is a triangle in $3D$ space (see figure below).\nThe distribution is parameterized by a positive $K$-dimensional vector $\\alpha$, with its probability density function defined as\n$$ p(x;\\alpha) = \\frac{1}{B(\\alpha)} \\prod_{k=1}^{K}x_k^{\\alpha_k - 1} $$\nwhere $B(\\cdot)$ is the famous beta function. The parameter $\\alpha$ governs how the density is distributed on the simplex space. For example, the picture below shows how the distribution is concentrated with different $\\alpha$ in the case of 2-simplex (brighter color denoting more dense areas).\nsource: Dirichlet distribution blog by Sue Liu It is noticeable that sample from a Dirichlet distribution is parameter of a categorical distribution. Thus, Dirichlet distribution is also seen as a distribution over categorical distribution. But why we need the Dirichlet distribution? It is because, in the context of Bayesian statistics, we want to control the uncertainty over some parameters rather than just a point estimate of them. To be more specific, given data $x$ with its likelihood function $f(x;\\theta)$, we want to infer the full distribution of $\\theta$ given $x$, but not an optimal point $\\theta^*$. Given a prior $p(\\theta)$, the posterior is proportional to the likelihood time the prior $$ p(\\theta|x) \\propto p(x|\\theta)p(\\theta) $$\nIf the posterior has the same functional form with the prior, the prior is said to be a conjugate prior to the likelihood. For example, the Dirichlet distribution is the conjugate prior to the categorical likelihood function. It means that, when the likelihood function is categorical and Dirichlet is chosen as a prior, then the posterior $$ p(\\theta|x;\\alpha) \\propto Cat(x|\\theta)Dir(\\theta;\\alpha) $$\nwill have the same form as the prior, which is a Dirichlet distribution. Conjugate prior makes it easy to calculate the posterior over parameter of interest $\\theta$. Thus, in the case of LDA where the categorical distribution is used to represent the topic distribution of each document and the word distribution of each topic, there is no better choice than Dirichlet distribution as a conjugate prior to control these categorical distributions.\nFigure _. The two probabilistic assumptions of LDA\n-- LDA is also a generative model. Hence, to understand its architecture clearly, we better see how it generates documents.\nGenerative process Suppose that we have $T$ topics and a vocabulary of $V$ words. Model LDA has 2 parameters $(\\alpha, \\beta)$ where\n $\\alpha$ denotes the Dirichlet prior that controls topic distribution of each document. $\\beta$ is a $2D$ matrix of size $T \\times V$ denotes word distribution of all topics ($\\beta_i$ is a word distribution of the i + 1th topic).  How a document is generated in LDA\nThe generative process can be pictured as above. Specifically,\n For each document $d$ with $N_d$ words  Sample document\u0026rsquo;s topic distribution $\\theta \\sim Dir(\\alpha)$ For each word positions $j$ from $1$ to $N_d$  Sample the topic of the current word $t_j \\sim Cat(\\theta)$   Sample the current word based on the topic $t_j$ and the word distribution parameters $\\beta$, $w_j \\sim Cat(\\beta_{t_j})$     Warning: $\\theta$ is now a latent variable, not model parameter. I keep the notation the same as the original paper for your ease of reference.\n The two problems of LDA LDA is a latent variable model, consisting of: observed data $w$; model parameters $\\alpha, \\beta$; and latent variables $z, \\theta$; as shown in the figure above. Hence, just like any typical latent variable model, LDA also have two problems needed to be solved.\nInference Given a document $d$ has $N$ words ${w_1^{(d)}, \u0026hellip;, w_N^{(d)}}$ and model parameters $\\alpha$, $\\beta$; infer the posterior distribution $p(z, \\theta| w^{(d)}; \\alpha, \\beta)$.\nWe can then use mean-field approximation to approximate $p(z, \\theta| w^{(d)}; \\alpha, \\beta)$, by introducing the mean-field variational distribution $q(z, \\theta; \\gamma, \\phi) = q(\\theta;\\gamma)\\prod_{i=1}^{N}q(z_i;\\phi_i)$. Deriving the ELBO to yield coordinate ascent update for each variational parameter is mathematically heavy so I will not put the mathematical stuff here. For reference, the derivation could be found in the Appendix of the original paper. Based on the coordinate ascent update, we obtain the optimal form for $q(\\theta;\\gamma)$ which is a Dirichlet distribution and each $q(z_i;\\phi_i)$ which is a categorical distribution. The coordinate ascent algorithm then return the optimal parameters $\\gamma^*, \\phi^*$.\nParameter estimation In LDA, the problem of parameter estimation is: find $\\alpha, \\beta$ that maximizes the likelihood function $$ p(w;\\alpha, \\beta) = \\int p(\\theta;\\alpha) \\prod_{i=1}^{N_d} \\sum_{t=0}^{T - 1} p(z_i = t|\\theta) p(w_i | \\beta, z_i=t) d\\theta $$\nSince the posterior $p(z, \\theta| w^{(d)}; \\alpha, \\beta)$ can not be computed exactly but can only be approximated (for instance, via variational inference in the previous section), we can not apply the EM algorithm directly to solve the estimation problem. To handle this, an algorithm named variational EM algorithm, which combines EM and mean-field inference, was introduced. Variational inference is now used in the E-step to compute the posterior, approximately. The algorithm used for LDA can be summarized as follows\n Initialize parameters $\\alpha, \\beta$ to $\\alpha^{(0)}, \\beta^{(0)}$ For each loop $t$ start from $0$  E step:  For each document $d$  Introduce the mean-field $q(z, \\theta; \\gamma, \\phi) = q(\\theta;\\gamma)\\prod_{i=1}^{N}q(z_i;\\phi_i)$ to approximate the posterior $p(z^{(d)}, \\theta^{(d)}|w^{(d)};{{\\alpha^{(t)}, \\beta^{(t)}}})$ Use coordinate ascent update algorithm to yield optimal $\\gamma^{(d)}, \\phi^{(d)}$     M step: Maximize the expected log-likelihood (up to some constant) with respect to $\\alpha, \\beta$ $$ \\mathop{max}_{{\\alpha^{(t+1)}, \\beta^{(t+1)}}} \\sum_{d=1}^{M} \\mathrm{E}_{z, \\theta \\sim q(z, \\theta; \\gamma^{(d)}, \\phi^{(d)})} {\\log p(w, z, \\theta ;{{\\alpha^{(t+1)}, \\beta^{(t+1)}}}}) $$  If the convergence standard is satisfied, stop     Note: Actually, there are many techniques to solve the two problems of LDA. Though, we only discuss about Variational EM in the scope of this blog :v\n References\n Latent Dirichlet Allocation ( pdf) Mean-field variational inference( pdf)  ","date":1584662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584662400,"objectID":"2159263ade13a9e8f4cddc091f74219d","permalink":"https://azraelzhor.github.io/post/lda-part-1/","publishdate":"2020-03-20T00:00:00Z","relpermalink":"/post/lda-part-1/","section":"post","summary":"Text data is everywhere. When having massive amounts of them, a need naturally arises is that we want them to be organized efficiently. A naive way is to organize them based on topics, meaning that text covering the same topics should be put into the same groups. The problem is that we do not know which topics a text document belongs to and manually labeling topics for all of them is very expensive.","tags":["latent dirichlet allocation","lda","topic modeling","em","variational inference","variational em"],"title":"Variational Expectation Maximization for Latent Dirichlet Allocation - Part 1","type":"post"},{"authors":[],"categories":null,"content":"","date":1583884800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583884800,"objectID":"84f7e0f8f8abacdb64bd35ec8662a16f","permalink":"https://azraelzhor.github.io/talk/object-detection/","publishdate":"2020-03-11T00:00:00Z","relpermalink":"/talk/object-detection/","section":"talk","summary":"","tags":["yolo","faster-rcnn"],"title":"Object Detection","type":"talk"},{"authors":null,"categories":null,"content":"Last year, I had a chance to be involved in an Advanced Computer Vision class held by a non-profit organization. During the class, object detection is one of the fields that I found myself interested. This motivated me to write a series of blogs in order to understand better some famous approaches that has been applied in the field. Though, the idea has been postponed until now :v. The first part of this series is about Faster RCN, one of the state-of-the-art methods used for object detection. In this blog post, I will walk you through the details of Faster RCNN. Hopefully, at the end of this blog, you would figure out the way Faster RCNN works.\nA little warm-up In object detection, we receive an image as input and localize bounding boxes, indicating various types of objects, as output.\nsource: https://highstonecorp.com/post/object-detection.html So what are bounding boxes? A bounding box is just a rectangle in the image. Its coordinates is defined as a tuple of $(x, y, w, h)$, where $(x, y)$ is the coordinate of the bounding box\u0026rsquo;s center and $w, h$ is its width, height, respectively. A bounding box is said to be best-fit an object if it is the smallest rectangle that fully encloses the object like the figure below.\nHence, we should label best-fit bounding box for the best quality of supervised data. In the next part, we will discuss Faster RCNN in detail.\nFaster RCNN architecture Faster RCNN, published in 2015, is the last of the RCNN trilogy (RCNN - Fast RCNN - Faster RCNN), which relies on proposed regions to detect objects. Though, unlike its predecessors which use selective search to find out the best regions, Faster RCNN makes use of neural network and \u0026ldquo;learn\u0026rdquo; to propose regions directly. These proposed regions is then fed into another neural network to be refined once again.\nLet us take a look at the overall architecture of Faster RCNN. The model comprises of $2$ modules\n The region proposal module takes feature map from a feature network and proposes regions. The Fast RCNN detector module takes those regions to predict the classes that the object belongs to.  Note that the feature network, which is VGG in the context of this blog, is shared between both modules. Also, to easily keep track of the story, let us follow a specific case in which we are given an image of shape $320\\times400\\times3$.\nFeature Shared Network The original paper uses ZF-net and VGG-net as feature network. Though, we only discuss VGG in the scope of this blog. The VGG receives an input image and produce a feature map with reduced spatial size. This size is determined by the net structure. In the case of VGG, the image spatial size is reduced $16$ times at the output layer. Hence, in our example, the feature map\u0026rsquo;s shape is $320/16 \\times400/16\\times512$, or $20 \\times 25 \\times 512$. The number $512$ is due to the number of filters in the last layer of VGG.\nThe feature map is then used for both region proposal network and region-based convolutional neural network, which will be discussed later.\nRegion Proposal Network (RPN) The goal of RPN is to propose regions that highly contain object. In order to do that, given the feature map, RPN does\n generate a predefined number of fixed-size anchors based on the feature map predict the objectness of each of these anchors refine their coordinates  Predefined anchors Specifically, for each pixel spatial location on the VGG feature map, we generate a predefined number of fixed size anchors. The shape of these anchor boxes are determined by a combination of predefined scales and edge ratios. In our example, if we use $3$ scales $64$, $128$, $256$ and $3$ edge ratios $1:1$, $1:2$, $2:1$, there will be $3*3=9$ type of anchors at each pixel location. A total of $20 * 25 * 9 = 4500$ anchors will be generated as a result. It is important to note that even though anchor boxes are created based on the feature map\u0026rsquo;s spatial location, they reference to the original input image, in which anchor boxes generated from the same feature map pixel location are centered at the same point on the original input, as illustrated in this figure below. RPN architecture The RPN is then designed to predict objectness of each anchor (classification) and refine its coordinates (regression). It consists of $3$ layers: one convolutional layer with $512$ filters of size $3 \\times 3$ followed by two sibling $1 \\times 1$ convolutional layers. These two sibling layers - one with $K$ filters and the other with $4K$ filters - allow for classification and regression, respectively. $K$ is determined as the number of generated anchors at each feature map location.\nIn our example when $K = 9$, after passing the VGG feature map through RPN, it produces a classification output with shape of $20 \\times 25 \\times 9$ and a regression output with shape of $20 \\times 25 \\times 36$. The total predictions of RPN will then have the shape of $20\\times25\\times45$, like the figure above.\nCreate labeled data for training RPN Label for classification Now, we need labeled data to train the RPN. For training classification task, each anchor box is labeled as\n positive if it contains object negative if it is background ignored if we want it to be ignored when training  based on the overlap with its nearest ground-truth bounding box.\nTo be more specific, we use the famous IoU (Intersection over Union) to measure the overlap. Let $p$ denotes the IoU between current anchor box and its nearest ground-truth bounding box. Then, we need to decide two thresholds $p_{neg}$, $p_{pos}$ for labelling. The labelling rule is then detailed as follows\n If $p \\geq p_{pos}$, label the bounding box as positive If $p \\leq p_{neg}$, label it as negative If $p_{neg} \u0026lt; p \u0026lt; p_{pos}$, ignore it when training  Label for regression The anchor box refinement is modeled as a regression problem, in which we predict the delta $({\\color{red}{t_x, t_y, t_w, t_h}})$ for each anchor box. This delta denotes the change needed to refine our predefined anchor boxes, as illustrated in this figure below\nFormally, we have\n $$\\begin{align} \\color{blue}{x} \u0026 = x_a + {\\color{red}{t_x}}*w_a \\\\ \\color{blue}{y} \u0026 = y_a + {\\color{red}{t_y}}*h_a \\\\ \\color{blue}{w} \u0026 = w_a * e^{\\color{red}{t_w}} \\\\ \\color{blue}{h} \u0026 = h_a * e^{\\color{red}{t_h}} \\end{align} $$  or\n $$\\begin{align} \\color{red}{t_x} \u0026 = ({\\color{blue}{x}} - x_a) / w_a \\\\ \\color{red}{t_y} \u0026 = ({\\color{blue}{y}} - y_a) / h_a \\\\ \\color{red}{t_w} \u0026 = log({\\color{blue}{w}}/w_a) \\\\ \\color{red}{t_h} \u0026 = log({\\color{blue}{h}}/h_a) \\end{align} $$  where $(x_a, y_a, w_a, h_a)$ denotes the anchor box\u0026rsquo;s coordinates and $({\\color{blue}{x, y, w, h}})$ denotes the refined box\u0026rsquo;s coordinates.\nTo create data for anchor regression training, we calculate the \u0026ldquo;ground-truth\u0026rdquo; delta $({\\color{red}{t_x^*, t_y^*, t_w^*, t_h^*}})$ based on each anchor box\u0026rsquo;s coordinates $(x_a, y_a, w_a, h_a)$ and its nearest ground-truth bounding box\u0026rsquo;s coordinates $({\\color{blue}{x^*, y^*, w^*, h^*}})$.\n $$ \\begin{align} \\color{red}{t_x^*} \u0026 = ({\\color{blue}{x^*}} - x_a) / w_a \\\\ \\color{red}{t_y^*} \u0026 = ({\\color{blue}{y^*}} - y_a) / h_a \\\\ \\color{red}{t_w^*} \u0026 = log({\\color{blue}{w^*}}/w_a) \\\\ \\color{red}{t_h^*} \u0026 = log({\\color{blue}{h^*}}/h_a) \\end{align} $$  Among those generated anchor boxes, the positive anchors are probably outnumbered by the negative ones. Thus, to avoid imbalanced classification, only a specific number of anchor boxes is used for training. In our example, among $4500$ anchor boxes generated, assume that we have $500$ \u0026ldquo;positive\u0026rdquo; anchor boxes, $2000$ \u0026ldquo;negative\u0026rdquo; anchor boxes and $2000$ \u0026ldquo;ignored\u0026rdquo; anchor boxes. Then, we only chose $256$ anchor boxes for training the RPN, including $128$ boxes of each type (\u0026ldquo;positive\u0026rdquo; and \u0026ldquo;negative\u0026rdquo;).\nRPN losses Regression Loss The smooth L1 loss is used for regression training. Its formulation is as below\n $$smooth_{L1}(x) = \\begin{cases} 0.5x^2 \u0026 \\mbox{if} \\; \\lvert x \\rvert where $x$ denotes the difference between prediction and ground truth $t - {\\color{blue}{t^*}}$. source: figure 3 in https://arxiv.org/pdf/1711.06753.pdf  The reason smooth L1 loss is preferred to L1 and L2 loss is because it can handle the problem of these two losses. Being quadratic for small values ($\\lvert x \\rvert \u0026lt; 1$) and linear for large values ($\\lvert x \\rvert \\geq 1$), smooth L1 loss is now less sensitive to outliers than L2 loss and also does not suffer from the problem of L1 loss, which is not differentiable around zero.\nClassification Loss For RPN binary classification, the binary cross-entropy loss is used.\nUse RPN to propose regions After training, we use RPN to predict the bounding box coordinates at each feature map location.\nAssume that the RPN predict $1000$ positive bounding boxes out of $4500$ anchor boxes. There are a lot of proposals. Hence, it is very likely that there are many bounding boxes referring to the same object, among those are predicted by RPN. This leads to redundant proposals, which can be eliminated by an algorithm known as non max suppression.\nNon-max suppression The idea of non max suppression is to filter out all but the box with highest confidence score for each highly-overlapped bounding box cluster (like the figure above), making sure that a particular object is identified only once.\nThe algorithm can be summarized as follows\n Given a list of proposals along with their confidence score, and a predefined overlap threshold  Initialize a list $L$ to contain bounding boxes. Sort the list, denoted by $S$, by confidence score in descending order Iterate through $S$, at each iteration  Compute the overlap between the current bounding box and the remain bounding boxes in $S$ Suppress all bounding boxes that have the computed overlap above the predefined threshold hold from $S$ Discard the current box from $S$, then move it to $L$   Return $L$    After non max suppression, we obtain some \u0026ldquo;good\u0026rdquo; bounding boxes in the input image. These boxes correspond with scaled regions in the VGG feature map. Then, these feature map patches are extracted as proposed regions, as shown in the figure below\nRegion-based Convolutional Neural Network Now we have proposed regions ready for the next phase. One notable problem arises here is that these proposed regions are not in the same shape, which make it difficult for neural network training. This is where we need RoI pooling layer to help construct fixed-size feature maps from these arbitrary-size regions.\nRoI Pooling To understand RoI pooling, let begin with a 2D example. No matter what the shape of the input slice is, a $2 \\times 2$ RoI pooling layer always transform the input to the output of size $2 \\times 2$ by\n Split the input into a $2 \\times 2$ matrix of roughly equal regions Do max pooling on each region  like this figure below (given input of shape $4 \\times 4$ or $5 \\times 5$).\nRoI used in Faster RCNN In Faster RCNN, we apply RoI pooling to a 3D proposed regions to obtain fixed-size regions. In our example, if $7\\times7$ RoI pooling is used, those fixed-size regions have the shape of $7\\times7\\times512$.\nDetection Network Those fixed-size feature maps from RoI pooling are then flattened and subsequently fed into a fully connected network for final detection. The net consists of $2$ fully connected layers of $4096$ neurons, followed by other $2$ sibling fully connected layers - one has $N$ neurons for classifying proposals and the other has $4*(N - 1)$ neurons for bounding box regression, where $N$ denotes the number of classes, including the background. Note that when a bounding box is classified as background, regression is unneeded. Hence, it makes sense that we only need $4*(N - 1)$ neurons for regression in total.\nIn our example, each $7\\times7\\times512$ feature map is fed to the detection net to produce the classification output has size of $4$, and the regression output has size of $12$.\nLabeled data for RCNN Label for classification Similar to the RPN, we make use of IoU metric to label data. Let $r$ now denotes the overlap between a refined anchor box produced by RPN and its nearest ground-truth anchor box. For each anchor box we label as follows\n if $r \\leq r_{min}$, label the proposed anchor box as background. if $r_{min} \u0026lt; r \u0026lt; r_{max}$, ignore it when training. if $r \\geq r_{max}$, label it as the class to which its nearest ground-truth box belongs.  where $r_{min}$ and $r_{max}$ are the two predefined thresholds.\nLabel for bounding box regression For regression, we also calculate the \u0026ldquo;ground-truth\u0026rdquo; deltas $({\\color{red}{t_x^*, t_y^*, t_w^*, t_h^*}})$ in the same fashion as those in RPN, but now based on each refined anchor box\u0026rsquo;s coordinates from the RPN $(x_r, y_r, w_r, h_r)$ and its nearest ground-truth bounding box\u0026rsquo;s coordinates $({\\color{blue}{x^*, y^*, w^*, h^*}})$.\nRCNN losses RCNN also uses smooth L1 loss for regression and categorical cross-entropy loss for classification.\nNow, we are done walking through Faster RCNN. Its entire architecture can be pictured as follows\n[TODO - introduce Colab links]\nReferences\n Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, 2015. ( arxiv)  ","date":1581724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581724800,"objectID":"b5e738e0afbd9d7feca0f89b9bd56e24","permalink":"https://azraelzhor.github.io/post/faster-rcnn/","publishdate":"2020-02-15T00:00:00Z","relpermalink":"/post/faster-rcnn/","section":"post","summary":"Last year, I had a chance to be involved in an Advanced Computer Vision class held by a non-profit organization. During the class, object detection is one of the fields that I found myself interested. This motivated me to write a series of blogs in order to understand better some famous approaches that has been applied in the field. Though, the idea has been postponed until now :v. The first part of this series is about Faster RCN, one of the state-of-the-art methods used for object detection.","tags":["object detection","rcnn","faster-rcnn"],"title":"Dive into Faster RCNN","type":"post"},{"authors":[],"categories":null,"content":"","date":1580860800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580860800,"objectID":"c72973cb0cd2fd24ced7220365c213f9","permalink":"https://azraelzhor.github.io/talk/generative-models/","publishdate":"2020-02-05T00:00:00Z","relpermalink":"/talk/generative-models/","section":"talk","summary":"","tags":["generative models","vae","gan","flow-based"],"title":"Generative Models","type":"talk"},{"authors":null,"categories":null,"content":"If you have been involving in machine learning for a while, you should have known about residual networks, which are proved to be powerful for image classification. Yet, apart from classification, they can be made invertible by some simple tricks to be used in other machine learning tasks as well. This family of residual networks called Invertible Residual Networks has been proposed recently by J Behrmann, 2018. In this blog post, I will walk you through the invention of invertible residual networks.\nThe motivation Classification only tells difference between data points. This is clearly not fulfilling for greedy human beings. We want a better understanding of data or, to be more specific, the data distribution itself. This underlying distribution of data is literally what the task of density estimation tries to estimate.\nDensity estimation Density estimation can be achieved through maximum likelihood estimation, in which we try to maximize the expected log-likelihood\n $$ \\mathbf{E}_{x \\sim p_{data}(x)}{p(x; \\theta)} $$  where $p_{data}(x)$ denotes the empirical distribution of observed data and $p(x; \\theta)$ denotes our assumed parametric distribution (simply known as model distribution).\nYet, when data is complex and high dimensional, a problem arises. It is hard to construct a parametric distribution which is not only expressive enough to capture the complexity of data but also tractable for maximum likelihood estimation. This hurdle can be overcome with the help of normalizing flows, in which we rely on them to construct our more complex model distribution from a simple latent prior. As a parametric bridge between these two distributions, normalizing flows allow for tractable data likelihood computation, which makes maximum likelihood estimation now possible.\nNormalizing Flows Normalizing flows were first introduced to solve the problem of density estimation. Though, it later became popular when introduced to deal with variational inference by [1]. The idea of normalizing flows is very simple that it transforms one distribution to another arbitrarily complex distribution, through a sequence of invertible mapping functions.\nA change of variables But, let\u0026rsquo;s first take a look at a change of variables rule, which forms the basis for normalizing flows.\nA change of variables\nGiven a random variable $x$ with its density function known as $p(x)$, if we map this variable using an invertible mapping $f$, so that $z = f(x)$ and $x = f^{-1}(z) ; \\forall x, z,$ then $z$ is still a random variable. Its normalized density function is then defined as follows\n$$ p(z) = p(x) \\left\\lvert \\det\\frac{\\partial f^{-1}}{\\partial z} \\right\\rvert = p(x) {\\left\\lvert \\det\\frac{\\partial f}{\\partial x} \\right\\rvert}^{-1}, \\tag{1} $$\nwhere the first equality is due to preservation of total probability of in both domain; and the second equality follows from the inverse function theorem.\nTaking logarithm of each side, we can rewrite $(1)$ as\n$$ \\ln p(z) = \\ln p(x) - \\ln \\left\\lvert \\det J_f(x) \\right\\rvert $$\nwhere $J_f$ denotes the Jacobian matrix of function $f$ evaluated at point $x$.\nA normalizing flow A normalizing flow\nWe can now form a normalizing flow by chaining together a finite sequence of these variable changes just described above. As an example, let us consider a flow in figure 2, in which we have $$z \\equiv z_K = f_K \\circ \u0026hellip; \\circ f_2 \\circ f_1 (x) \\equiv F(x)$$\nBy consecutively applying variables change formula $(2)$, we get\n $$ \\begin{align} \\ln p(z) = \\ln p(z_K) \u0026 = \\ln p(z_{K-1}) - \\ln \\left\\lvert \\det J_{f_K}(z_{K-1})\\right\\rvert \\\\ \u0026 = \\ln p(z_{K-2}) - \\sum_{k=K-1}^{K}\\ln \\left\\lvert \\det J_{f_k}(z_{k-1})\\right\\rvert \\\\ \u0026 = \\;... \\\\ \u0026 = \\ln p(x) - \\sum_{k=1}^{K} \\ln \\left\\lvert \\det J_{f_k}(z_{k-1})\\right\\rvert \\\\ \\end{align} $$  Continuing the derivation we get\n $$ \\begin{align} \\ln p(z) \u0026 = \\ln p(x) - \\ln \\left\\lvert \\prod_{k=1}^{K} \\det J_{f_k}(z_{k-1})\\right\\rvert \\\\ \u0026 = \\ln p(x) - \\ln \\left\\lvert \\det \\prod_{k=1}^{K} J_{f_k}(z_{k-1})\\right\\rvert \\ (\\because \\det(AB) = \\det(A)\\det(B)) \\\\ \u0026 = \\ln p(x) - \\ln \\left\\lvert \\det J_F(x)\\right\\rvert \\ (\\textrm{because of derivative chain rule})\\\\ \\end{align} $$  It is easy to realize that the last equation is literally a variables change formula with transformation $F$. This does make sense because a normalizing flow can also be viewed as a change of variables but with a much more complex invertible transformation. Here, $F$ is clearly invertible as it is a composition of an arbitrary number of invertible functions.\nBy designing an appropriate $F$, we can obtain an arbitrarily complex normalized density function at the completion of a normalizing flow. Hence, normalizing flows can be intuitively interpreted as a systematic way to distort the input density function, making it more complex (like in variational inference setting) or simpler (like in density estimation setting). However, in order for normalizing flows to be useful in practice, we need to have two conditions satisfied:\n Those transformation functions $f_k$ obviously need to be invertible. The determinant of their Jacobian matrices $J_{f_k}$ need to be easy to compute, in order to obtain a tractable likelihood.  In fact, many approaches have been proposed to construct those easy-to-use transformation functions lately. Inspired by normalizing flows, the authors of the paper has also managed to exploit residual networks as transformation functions used for normalizing flows. Thus, before diving into the details, let take a look back at the architecture of residual networks.\nThe awesome residual block Residual network is composed of a sequence of residual blocks, with each block can be simplified as this figure below\nA residual block\nAs we can see, each residual block consists of a residual part denoted by $g$ and an identity part. From mathematical viewpoint, each block can be clearly counted as a function which takes input $x$ and produces $z$ as output. Formally, we have $$ z = f(x) = g(x) + x \\tag{1} $$\nBack to our main story, it is obvious that the goal is to make use of residual network as a transformation function for normalizing flows. Since residual network can be interpreted as a composition function of multiple residual blocks, making each individual block invertible is a sufficient condition for the invertibility of the whole net.\nMaking the nets invertible In the inverse phase, each block takes $z$ as input and produces $x$ as output. Thus, in order for a block to be invertible, we need to enforce the existence and uniqueness of output $x$ for each input $z$.\nFrom $(1)$, we have $$ x = z - g(x) $$\nLet define $h(x) = z - g(x)$ to be a function of $x$, where z acts as a constant. The requirement can now be formulated as follows: The equation $x = h(x)$ must have only one root or, to put it in a formal way, $h(x)$ has a unique fixed point.\n Fixed point: Let X be a metric space and let T: X → X be a mapping in X. A fixed point of T is a point in X such that T(x) = x.\n Fortunately, this requirement can be obtained according to the famous Banach fixed-point theorem.\nFixed point theorem The Banach fixed point theorem, also called contraction mapping theorem, states that every contraction mapping in a complete metric space admits a unique fixed point.\n Contraction mapping: Let $(M, d)$ be a complete metric space. A function $T$: $M$ → $M$ is a contraction mapping if there exists a real number $0 \\leq k \u0026lt; 1$ such that:\n$$\\quad d(T(x), T(y)) \\leq k d(x, y) , \\quad \\quad \\forall x, y \\in M$$\nThe smallest $k$ for which the above inequality holds is called the Lipschitz constant of $f$, denoted by $Lip(T)$\n Banach theorem: Let $(M, d)$ be a complete metric space and $T$: $M$ → $M$ be a contraction mapping. Then T has a unique fixed point $x \\in M$. Furthermore, if $y \\in M$ is arbitrary chosen, then the iterates ${ {x_n}}_{n=0}^\\infty$, given by\n $$ \\begin{align} \u0026 x_0 = y \\\\ \u0026 x_n = T(x_{n-1}), n \\geq 1, \\end{align} $$  have the property that $\\lim_{n\\to\\infty} x_n = x$ as illustrated in the figure below.\nFixed point convergence illustration in Banach theorem\nEnforcing Lipschitz constraint Based on the Banach theorem above, our enforcing condition then becomes\n$$ Lip(h) \u0026lt; 1 ;\\textrm{or}; Lip(g) \u0026lt; 1 $$\nHence $g$ can be implemented as a composition of contractive linear or nonlinear mappings like the figure below.\nContractive residual mapping\n For nonlinear mappings, ReLU, ELU and tanh are the possible choices for contraction constraint. For linear mappings, implemented as convolutional layers $W_i$, they can be made contractive by satisfying the condition $$\\lVert W_i \\rVert_2 \u0026lt; 1 \\quad \\forall W_i$$ where $\\lVert a \\rVert_2$ denotes the spectral norm of matrix a.   Spectral norm of a matrix: The largest singular value of a matrix.\n The spectral norm of non-square matrix $W_i$ can be directly estimated using the power iteration algorithm (by Gouk et el. 2018), which yields an underestimate $\\tilde \\sigma_i \\leq \\lVert W_i \\rVert_2$. The algorithm can be summarized as follows:\n $$ \\begin{align} \u0026 \\textrm{Initialize} \\; x_0 \\\\ x_k \u0026 = W_i^T W_i x_{k - 1}, \\; \\forall k, 1 \\leq k \\leq n \\\\ \\tilde \\sigma_i \u0026 = \\frac{\\lVert W_i x_n\\rVert_2}{\\lVert x_n \\rVert_2} \\\\ \\end{align} $$  We then normalize the parameter $W_i$ by\n $$ \\tilde{W_i} = \\begin{cases} \\frac{cW_i}{\\tilde \\sigma_i}, \u0026 \\mbox{if} \\; \\frac{c}{\\tilde \\sigma_i} where $c$ is the hyperparameter ($c How to yield the inverse output Though the constraint above guarantees invertibility of the residual network, it does not provide any analytical form for the inverse. Fortunately, inverse output of each residual block can be yielded through a simple fixed-point iteration, as described below\n Initialize value $x = x_0$ For each iteration $i$, $x_{i+1} = h(x_i) = z - g(x_i)$  Computing the likelihood So far, we have managed to construct invertible residual networks. We can now make use of them as a transformation for density estimation.\ninvertible residual network as transformation function in normalizing flows\nBut there is still one problem need to be dealt with. In order for the likelihood to be tractable, we need to compute the determinant of the Jacobian matrix of the residual network $F$ or, instead, the determinant of the Jacobian matrix of each residual block $f$. $$\\ln p(x) = \\ln p(z) + \\ln \\left\\lvert \\det J_F(x)\\right\\rvert = \\ln p(z) + \\sum_{k=1}^{K} \\ln \\left\\lvert \\det J_{f_k}(z_{k-1})\\right\\rvert$$ The computation of the determinant of full Jacobian matrix requires $O(d^3)$ time, which makes it prohibitive for high-dimensional data like image. Fortunately, we can approximate the term in a certain way.\nThe log determinant term For each residual block $f$, we have\n $$ \\begin{align} \\ln \\left\\lvert \\det J_f(x)\\right\\rvert \u0026 = \\ln (\\det J_f(x)) \\textrm{( $\\det J_f$ is always positive)} \\\\ \u0026 = tr(\\ln J_f(x)) \\textrm{($\\ln \\det A = tr(\\ln(A))$)} \\\\ \u0026 = tr(ln\\frac{\\partial f}{\\partial x}) \\\\ \u0026 = tr(ln\\frac{\\partial (x + g(x))}{\\partial x})\\\\ \u0026 = tr(ln\\ (I + J_g(x))) \\textrm{($I$ denotes identity matrix)} \\\\ \u0026 = tr(\\sum_{k=1}^\\infty(-1)^{k + 1}\\frac{J_g^k}{k}) \\textrm{(power series expression of matrix logarithm)} \\\\ \u0026 = \\sum_{k=1}^\\infty(-1)^{k + 1}\\frac{tr(J_g^k)}{k} \\textrm{($tr(A + B) = tr(A) + tr(B)$)} \\\\ \\end{align} $$   Matrix logarithm and its power series expression: A logarithm of matrix $M$ is any matrix $X$ such that $e^X = M$. It can be expressed as a power series $$ln(M) = \\sum_{k=1}^\\infty(-1)^{k + 1}\\frac{(M - I)^k}{k}$$ whenever the series converges.\n Now the log-determinant term has been rewritten as an infinite sum of traces of matrix powers, which makes it easier to approximate. Even though, there is still a bunch of drawbacks if we want to approximate the term:\n Computing $tr(J_g)$ costs $O(d^2)$ Computing matrix powers $J_g^k$ requires knowledge of full Jacobian The series is infinite  Hutchinson trace estimator Evaluating the trace of matrix powers $J_g^k$ is expensive due to full knowledge of Jacobian matrix and also matrix-matrix multiplications, hence comes the Hutchinson method for trace approximation.\nHutchinson trace estimator is a Monte Carlo approach to approximate the trace of matrix powers, for example $J_g^k$ in our case, without fully evaluating them. Specifically, a random vector $v$ is introduced to estimate the trace $$tr(A) = \\mathrm{E}_{v \\sim p(v)}v^{T}Av $$\nwith the constraint that $v$ is drawn from a fixed distribution $p(v)$, satisfying $\\mathrm{E}[v] = 0$ and $\\mathrm{Var}[v] = I$. Hence it is obvious that the Gaussian $N(0, I)$ is a good choice for $p(v)$. Applying the trace estimator, we have\n$$ tr(J_g^k) = \\mathrm{E}_{v \\sim N(0, I)} v^T J_g^k v $$\nThe matrix power computation can be circumvented by evaluating $v^T J_g^k v$ in a recursive fashion\n $$ \\begin{align} w_0 \u0026 = v \\\\ w_k \u0026 = J_g w_{k - 1}, \\forall k \\geq 1 \\\\ v^T J_g^k v \u0026 = v^T w_k \\\\ \\end{align} $$  which requires now only matrix-vector multiplication. Furthermore, the term $w_k$ can be evaluated roughly as the same cost as evaluating $g$ using reverse-mode automatic differentiation, alleviating the heavy computation of evaluating $J_g$ explicitly.\nNow, the only problem remains is the computation of infinite series, which can be addressed by truncating the series at a finite index $n$\n$$ \\ln |\\det J_f(x)| \\approx \\sum_{k=1}^{n}(-1)^{k + 1}\\frac{tr(J_g^k)}{k} $$\nImplementation Original implementation by the paper\u0026rsquo;s author( link)\nA TensorFlow implementation by me( link)\nReferences\n  Variational Inference with Normalizing Flows  Invertible Residual Networks  Normalizing Flows: An Introduction and Review of Current Methods  High-Dimensional Probability Estimation with Deep Density Models  Hutchinson\u0026rsquo;s Trick  Spectral Normalization Explained  Regularisation of Neural Networks by Enforcing Lipschitz Continuity  Large-scale Log-determinant Computation through Stochastic Chebyshev Expansions  ","date":1579392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579392000,"objectID":"b06c8416d3613a34de7f2d674b8607f1","permalink":"https://azraelzhor.github.io/post/invertible-residual-networks/","publishdate":"2020-01-19T00:00:00Z","relpermalink":"/post/invertible-residual-networks/","section":"post","summary":"If you have been involving in machine learning for a while, you should have known about residual networks, which are proved to be powerful for image classification. Yet, apart from classification, they can be made invertible by some simple tricks to be used in other machine learning tasks as well. This family of residual networks called Invertible Residual Networks has been proposed recently by J Behrmann, 2018. In this blog post, I will walk you through the invention of invertible residual networks.","tags":["generative model","invertible residual networks","normalizing flows","computer vision"],"title":"Invertible Residual Networks for Generative Modeling","type":"post"},{"authors":[],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"f1d7a40a7d6a4cfffd00e4b2a00ce3c0","permalink":"https://azraelzhor.github.io/talk/vietnamese-data-augmentation/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/talk/vietnamese-data-augmentation/","section":"talk","summary":"","tags":["nlp","data augmentation","backtranslation"],"title":"Vietnamese Data Augmentation","type":"talk"}]