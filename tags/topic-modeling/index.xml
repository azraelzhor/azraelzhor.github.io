<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>topic modeling | Thang Le</title>
    <link>https://azraelzhor.github.io/tags/topic-modeling/</link>
      <atom:link href="https://azraelzhor.github.io/tags/topic-modeling/index.xml" rel="self" type="application/rss+xml" />
    <description>topic modeling</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>joocxi@2020</copyright><lastBuildDate>Tue, 12 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=true) shape:circle]</url>
      <title>topic modeling</title>
      <link>https://azraelzhor.github.io/tags/topic-modeling/</link>
    </image>
    
    <item>
      <title>Variational Expectation Maximization for Latent Dirichlet Allocation - Part 2</title>
      <link>https://azraelzhor.github.io/post/lda-part-2/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      <guid>https://azraelzhor.github.io/post/lda-part-2/</guid>
      <description>&lt;p&gt;After discussing LDA in the previous part, we now get our hands dirty by implementing the Variational EM algorithm. Recall that the goal is to estimate $\alpha, \beta$ that maximizes the ELBO&lt;/p&gt;
&lt;div&gt;
$$
\mathrm{E}_{q}\log p(w, \theta, z;\alpha, \beta) - \mathrm{E}_{q}\log {q(z, \theta;\gamma, \phi)}
$$
&lt;/div&gt;
&lt;p&gt;where the joint likelihood and the variational distribution are factorized as follows&lt;/p&gt;
&lt;p&gt;$$
p(w, \theta, z; \alpha, \beta) = Dir(\theta;\alpha)\prod_{n=1}^{N}Cat(z_n;\theta) Cat(w_n;z_n, \beta)
$$&lt;/p&gt;
&lt;p&gt;$$
q(z, \theta; \gamma, \phi) = Dir(\theta;\gamma)\prod_{n=1}^{N}Cat(z_n;\phi_n)
$$&lt;/p&gt;
&lt;h2 id=&#34;working-with-the-elbo&#34;&gt;Working with the ELBO&lt;/h2&gt;
&lt;p&gt;But before getting into code, we need to derive the ELBO. Substituting these factorizations into the ELBO, we obtain&lt;/p&gt;
&lt;div&gt;
\begin{align}
L &amp; = \mathrm{E}_{q}\log p(w, \theta, z;\alpha, \beta) - \mathrm{E}_{q}\log {q(z, \theta;\gamma, \phi)} \\
&amp; = \mathrm{E}_{q}\log Dir(\theta; \alpha) + \sum_{n=1}^{N} \Big[ \mathrm{E}_{q} \log Cat(z_n; \theta) + \mathrm{E}_{q} \log Cat(w_n; z_n, \beta) \Big] \\
&amp; \quad - \mathrm{E}_{q} \log Dir(\theta; \gamma) - \sum_{n=1}^{N}\mathrm{E}_{q} \log Cat (z_n;\phi_n) \tag{1}
\end{align}
&lt;/div&gt;
&lt;h3 id=&#34;dealing-with-expected-values&#34;&gt;Dealing with expected values&lt;/h3&gt;
&lt;p&gt;To handle the expectations in the ELBO, we need to rewrite the Dirichlet distribution in exponential form as follows&lt;/p&gt;
&lt;div&gt;
\begin{align}
Dir(x;\alpha) &amp; = \frac{1}{B(\alpha)} \prod_{k=1}^{K}x_k^{\alpha_k - 1} \\
&amp; = \frac{\Gamma(\sum_{k=1}^{K}\alpha_k)}{\sum_{k=1}^{K}\Gamma(\alpha_k)} \prod_{k=1}^{K}x_k^{\alpha_k - 1} \\
&amp; = \exp\Big[ \sum_{k=1}^{K} (\alpha_k - 1) \log x_k + \log \Gamma(\sum_{k=1}^{K}\alpha_k) - \sum_{k=1}^{K}\log\Gamma(\alpha_k) \Big]
\end{align}
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Exponential family distribution&lt;/strong&gt;
$$
p(x|\theta) = h(x) exp(\eta \cdot T(x) - A(\eta))
$$&lt;/p&gt;
&lt;p&gt;where $h(x)$ is known as &lt;strong&gt;base measure&lt;/strong&gt;, $\eta(\theta)$ is &lt;strong&gt;natural parameter&lt;/strong&gt;, $T(x)$ is &lt;strong&gt;sufficient statistic&lt;/strong&gt; and $A(\theta)$ is &lt;strong&gt;log normalizer&lt;/strong&gt;. One important property of the exponential family is that the mean of the sufficient statistic $T(x)$ can be derived by differentiating the natural parameter $A(\eta)$
$$
E[T_j]= \frac{\partial A(\eta)}{\partial \eta_j} \tag{2}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Applying property $(2)$ for the case of the Dirichlet distribution, we have&lt;/p&gt;
&lt;div&gt;
\begin{align}
\mathrm{E}_{x \sim Dir(x;\alpha)}\log x_k &amp; = \frac{\partial(\sum_{j=1}^{K}\log\Gamma(\alpha_j) - \log \Gamma(\sum_{j=1}^{K}\alpha_j))  }{\partial (\alpha_k - 1)} \\
&amp; = \frac{\partial(\sum_{j=1}^{K}\log\Gamma(\alpha_j) - \log \Gamma(\sum_{j=1}^{K}\alpha_j))  }{\partial \alpha_k} \\
&amp; = \Psi(\alpha_k) - \Psi(\sum_{j=1}^{K}\alpha_j)
\end{align}
&lt;/div&gt;
&lt;p&gt;where $\Psi(\cdot)$ is the derivative of the logarithm of Gamma function (also known as Digamma).&lt;/p&gt;
&lt;p&gt;Also, the categorical distribution can be represented using 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Iverson_bracket&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Iverson bracket&lt;/a&gt; $[\cdot]$&lt;/p&gt;
&lt;p&gt;$$
Cat(x;\theta) = \prod_{i=1}^K \theta_i^{[x=i]}
$$&lt;/p&gt;
&lt;p&gt;where $[x=i]$ evaluates to  $1$ if $x = i$, $0$ otherwise (with the assumption that values of $x$ fall into the range ${1, 2, &amp;hellip;, K}$).&lt;/p&gt;
&lt;!-- $$
\log Cat(x;\theta) = \sum_{i=1}^K [x=i] \log \theta_i
$$ --&gt;
&lt;p&gt;Expectation of a function $f(x)$ with respect to the categorical distribution is computed as&lt;/p&gt;
&lt;div&gt;
$$
\mathrm{E}_{x\sim Cat(x;\theta)} f(x) = \sum_{i=1}^{K} \theta_i f(i)
$$
&lt;/div&gt;
&lt;h3 id=&#34;deriving-the-elbo&#34;&gt;Deriving the ELBO&lt;/h3&gt;
&lt;p&gt;Using these results above, we have&lt;/p&gt;
&lt;div&gt;
\begin{align}
&amp; \mathrm{E}_{q}\log Dir(\theta; \alpha) \\
&amp; = \mathrm{E}_{q} \Big[\sum_{k=1}^{K} (\alpha_k - 1) \log \theta_k + \log \Gamma(\sum_{k=1}^{K}\alpha_k) - \sum_{k=1}^{K}\log\Gamma(\alpha_k)\Big] \\
&amp; = \sum_{k=1}^{K}(\alpha_k - 1) \mathrm{E}_{\theta \sim Dir(\theta;\gamma)} \log\theta_k + \log \Gamma(\sum_{k=1}^{K}\alpha_k) - \sum_{k=1}^{K}\log\Gamma(\alpha_k) \\
&amp; = \sum_{k=1}^{K}(\alpha_k - 1) (\Psi(\gamma_k) - \Psi(\sum_{j=1}^{K}\gamma_j)) + \log \Gamma(\sum_{k=1}^{K}\alpha_k) - \sum_{k=1}^{K}\log\Gamma(\alpha_k) \tag{3} \\
&amp;\\
&amp; \mathrm{E}_{q} \log Cat(z_n; \theta) \\
&amp; = E_{z_n \sim Cat(z_n;\phi_n), \theta \sim Dir(\theta;\gamma)} \log Cat(z_n;\theta) \\
&amp; = E_{z_n \sim Cat(z_n;\phi_n), \theta \sim Dir(\theta;\gamma)} \sum_{j=1}^{K}[z_n=j]\log \theta_j \\
&amp; = \sum_{i=1}^{K} \phi_{ni} \mathrm{E}_{\theta \sim Dir(\theta; \gamma)} \sum_{j=1}^{K}[i=j]\log \theta_j \\
&amp; = \sum_{i=1}^{K} \phi_{ni} \mathrm{E}_{\theta \sim Dir(\theta; \gamma)} \log \theta_i \\
&amp; = \sum_{i=1}^{K} \phi_{ni}(\Psi(\gamma_i) - \Psi(\sum_{j=1}^{K}\gamma_j)) \tag{4} \\
&amp;\\
&amp; \mathrm{E}_{q} \log Cat(w_n;z_n, \beta) \\
&amp; = \mathrm{E}_{z_n \sim Cat(z_n;\phi_n)} \log Cat(w_n;\beta_{z_n}) \\
&amp; = \mathrm{E}_{z_n \sim Cat(z_n;\phi_n)} \sum_{j=1}^{V} [w_n=j] \log \beta_{z_n j} \\
&amp;\quad \textrm{(assumming that $w_n$ represents the index of word in the vocabulary)} \\
&amp; = \sum_{i=1}^{K} \phi_{ni} \sum_{j=1}^{V} [w_n=j] \log \beta_{ij}\\
&amp; = \sum_{i=1}^{K} \sum_{j=1}^{V} \phi_{ni} [w_n=j] \log \beta_{ij} \tag{5} \\
&amp;\\
&amp; \mathrm{E}_{q} \log Dir(\theta;\gamma) \\
&amp; = \mathrm{E}_{q} \Big[\sum_{k=1}^{K} (\gamma_k - 1) \log \theta_k + \log \Gamma(\sum_{k=1}^{K}\gamma_k) - \sum_{k=1}^{K}\log\Gamma(\gamma_k)\Big] \\
&amp; = \sum_{k=1}^{K}(\gamma_k - 1) \mathrm{E}_{\theta \sim Dir(\theta;\gamma)} \log\theta_k + \log \Gamma(\sum_{k=1}^{K}\gamma_k) - \sum_{k=1}^{K}\log\Gamma(\alpha_k) \\
&amp; = \sum_{k=1}^{K}(\gamma_k - 1) (\Psi(\gamma_k) - \Psi(\sum_{j=1}^{K}\gamma_j)) + \log \Gamma(\sum_{k=1}^{K}\gamma_k) - \sum_{k=1}^{K}\log\Gamma(\gamma_k) \tag{6} \\
&amp;\\
&amp; \mathrm{E}_{q} \log q(z_n; \phi_n) \\
&amp; = \mathrm{E}_{z_n \sim Cat(z_n;\phi_n)} \log Cat(z_n;\phi_n) \\
&amp; = \mathrm{E}_{z_n \sim Cat(z_n;\phi_n)} \sum_{j=1}^{K}[z_n = j]\log \phi_{nj} \\
&amp; = \sum_{i=1}^{K} \phi_{ni} \sum_{j=1}^{K} [i=j] \log \phi_{nj} \\
&amp; = \sum_{i=1}^{K} \phi_{ni} \log \phi_{ni} \tag{7}
\end{align}
&lt;/div&gt;
&lt;p&gt;Substituting $(3), (4), (5), (6), (7)$ into $(1)$, the ELBO becomes&lt;/p&gt;
&lt;div&gt;
\begin{align}
L&amp;(\gamma, \phi;\alpha, \beta) \\
= &amp;\sum_{k=1}^{K}(\alpha_k - 1) (\Psi(\gamma_k) - \Psi(\sum_{j=1}^{K}\gamma_j)) + \log \Gamma(\sum_{k=1}^{K}\alpha_k) - \sum_{k=1}^{K}\log\Gamma(\alpha_k) \\
&amp; + \sum_{n=1}^{N} \sum_{i=1}^{K} \phi_{ni}(\Psi(\gamma_i) - \Psi(\sum_{j=1}^{K}\gamma_j)) \\
&amp; + \sum_{n=1}^{N} \sum_{i=1}^{K} \sum_{j=1}^{V} \phi_{ni} [w_n = j]\log \beta_{ij}\\
&amp; - \sum_{k=1}^{K}(\gamma_k - 1) (\Psi(\gamma_k) - \Psi(\sum_{j=1}^{K}\gamma_j)) - \log \Gamma(\sum_{k=1}^{K}\gamma_k) + \sum_{k=1}^{K}\log\Gamma(\gamma_k) \\
&amp; - \sum_{n=1}^{N} \sum_{i=1}^{K} \phi_{ni} \log \phi_{ni} \tag{8}
\end{align}
&lt;/div&gt;
which is now much easier to deal with.
&lt;h2 id=&#34;preparing-data&#34;&gt;Preparing data&lt;/h2&gt;
&lt;p&gt;Now we dive into the code. For illustration purpose, we use a public dataset from Kaggle. The dataset contains news headlines crawled from ABC News. Here is some code to load the data&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;news_data_path = &amp;quot;abcnews-date-text.csv&amp;quot;

gdown.download(&amp;quot;https://drive.google.com/uc?id=1BGaMi0XURByE0WM4omDwskoq83WnTXyx&amp;quot;,
               news_data_path,
               quiet=False)

data_df = pd.read_csv(news_data_path, error_bad_lines=False);
data_df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A small piece of the data will look like this&lt;/p&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}

td, th, tr {
    border: 1px solid #ddd;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;0&#34; class=&#34;dataframe&#34; style=&#34;display:table;&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;publish_date&lt;/th&gt;
      &lt;th&gt;headline_text&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;20030219&lt;/td&gt;
      &lt;td&gt;aba decides against community broadcasting lic...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;20030219&lt;/td&gt;
      &lt;td&gt;act fire witnesses must be aware of defamation&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;20030219&lt;/td&gt;
      &lt;td&gt;a g calls for infrastructure protection summit&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;20030219&lt;/td&gt;
      &lt;td&gt;air nz staff in aust strike for pay rise&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;20030219&lt;/td&gt;
      &lt;td&gt;air nz strike to affect australian travellers&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h2 id=&#34;preprocessing-data&#34;&gt;Preprocessing data&lt;/h2&gt;
&lt;p&gt;There is a total of $1186018$ headlines in the original dataset but for a quick experiment, we extract the very first $10000$ headlines only&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data = data_df[&amp;quot;headline_text&amp;quot;][:10000]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we need to do some preprocessing stuff&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remove the stop words using &lt;code&gt;stopwords&lt;/code&gt; from &lt;code&gt;nltk&lt;/code&gt; package&lt;/li&gt;
&lt;li&gt;Build the vocabulary with &lt;code&gt;word2idx&lt;/code&gt; and &lt;code&gt;idx2word&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create the &lt;code&gt;corpus&lt;/code&gt; containing all documents&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;corpus = []
word2idx = {}
idx2word = {}

for line in data:
    doc = [w for w in line.split(&#39; &#39;) if w not in stopwords.words()]
    for word in doc:
        if word not in word2idx:
            word2idx[word] = len(word2idx)
            idx2word[len(idx2word)] = word

    corpus.append(doc)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;global-configuration&#34;&gt;Global configuration&lt;/h2&gt;
&lt;p&gt;Next, we set up some global configuration before implementing LDA model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;max_doc_length = 0
for doc in corpus:
    if max_doc_length &amp;lt; len(doc):
        max_doc_length = len(doc)

class Config:
    corpus = corpus
    word2idx = word2idx
    idx2word = idx2word
    num_vocabs = len(word2idx) # V
    max_doc_length = max_doc_length # N
    va_threshold = 1e-6 # threshold for variational infrence
    em_threshold = 1e-4 # threshold for variational EM
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;lda-model-definition&#34;&gt;LDA model definition&lt;/h2&gt;
&lt;p&gt;We then define an &lt;code&gt;LDA&lt;/code&gt; class to handle the main logic of Variational EM&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class LDA(object):
    def __init__(self,
                 corpus,
                 num_topics,
                 num_words,
                 num_vocabs,
                 word2idx,
                 idx2word):

        self.corpus = corpus # collection of documents
        self.K = num_topics # number of topics in total
        self.V = num_vocabs # number of vocabulary

        self.word2idx = word2idx
        self.idx2word = idx2word

        # model parameters
        self.alpha = None
        self.beta = None

        # sufficient statistics
        self.beta_ss = None
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;compute-the-log-likelihood&#34;&gt;Compute the log-likelihood&lt;/h2&gt;
&lt;p&gt;Evaluating $(8)$ requires the computation of Gamma and Digamma functions. Fortunately, we can make use of the &lt;code&gt;scipy&lt;/code&gt; package to handle the heavy work.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.special import digamma, loggamma
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then implement the &lt;code&gt;log_likelihood&lt;/code&gt; function for current document and variational parameters $\gamma$, $\phi$; given model parameters $\alpha, \beta$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def log_likelihood(doc,
                   gamma,
                   phi):
    &amp;quot;&amp;quot;&amp;quot;
    Compute the (approximate) log-likelihood
    &amp;quot;&amp;quot;&amp;quot;
    # (K,)
    digamma_derivative = digamma(gamma) - digamma(np.sum(gamma))

    l1 = loggamma(np.sum(self.alpha)) - np.sum(loggamma(self.alpha)) \
        + np.sum((self.alpha - 1) * digamma_derivative) \
        - loggamma(np.sum(gamma)) + np.sum(loggamma(gamma)) \
        - np.sum((gamma - 1) * digamma_derivative)

    l2 = 0
    for i in range(self.K):
        for n in range(len(doc)):
            if phi[n, i] &amp;gt; 0:
                l2 += phi[n, i] * (digamma_derivative[i] \
                    + np.log(self.beta[i, self.word2idx[doc[n]]]) \
                    - np.log(phi[n, i]))

    return l1 + l2
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;variational-inference&#34;&gt;Variational Inference&lt;/h2&gt;
&lt;p&gt;The goal of variational inference is to find the optimal $\phi^*, \gamma^*$ for the mean-field distribution $q(z, \theta; \gamma, \phi)$. By taking derivatives of $(8)$ with respect to $\phi, \gamma$ and set it to zero, we obtain coordinate updates
$$
\phi_{ni} \propto \beta_{iv} \exp(\Psi(\gamma_i) - \Psi(\sum_{j=1}^{K}\gamma_j))
$$&lt;/p&gt;
&lt;p&gt;$$
\gamma_i = \alpha_i + \sum_{n=1}^{N} \phi_{ni}
$$&lt;/p&gt;
&lt;p&gt;where $v$ denotes the unique index of the word $w_n$ in the vocabulary. For more detailed derivation of these updates, we refer to Appendix A (section 3.1, 3.2) of the original LDA paper. Then, we can implement the coordinate ascent algorithm for variational inference as follows&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def variational_inference(self, doc):
    &amp;quot;&amp;quot;&amp;quot;
    Do the variational inference for each document
    &amp;quot;&amp;quot;&amp;quot;

    N = len(doc)
    # init variational parameters
    # (N, K)
    phi = np.full(N, self.K), 1.0 / self.K)
    # (K,)
    gamma = self.alpha + N * 1.0 / self.K

    old_likelihood = -math.inf

    # coordinate ascent
    while True:
        # update phi
        for n in range(N):
            for i in range(self.K):
                phi[n, i] = self.beta[i, self.word2idx[doc[n]]] \
                * np.exp(digamma(gamma[i]))

        # normalize phi   
        phi = phi / np.sum(phi, axis=1, keepdims=True)

        # update gamma
        gamma = self.alpha + np.sum(phi, axis=0)

        likelihood = self.log_likelihood(doc, gamma, phi)

        converged = (old_likelihood - likelihood) / likelihood

        old_likelihood = likelihood

        if converged &amp;lt; cfg.va_threshold:
            break

    return phi, gamma, likelihood
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;variational-em&#34;&gt;Variational EM&lt;/h2&gt;
&lt;p&gt;Recall that the Variational EM algorithm consisting of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize parameters $\alpha^{(0)}, \beta^{(0)}$&lt;/li&gt;
&lt;li&gt;For each loop $t$ start from $0$
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;E step&lt;/strong&gt;: For each document $d$, obtain the optimal $\gamma^{(d)}, \phi^{(d)}$ of the variational distribution $q(z, \theta; \gamma, \phi) = q(\theta;\gamma)\prod_{n=1}^{N}q(z_n;\phi_n)$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M step&lt;/strong&gt;: Maximize the expected log-likelihood (up to some constant)
  &lt;div&gt;$$
  \mathop{max}_{{\alpha^{(t+1)}, \beta^{(t+1)}}} \sum_{d=1}^{M} \mathrm{E}_{z, \theta \sim q(z, \theta; \gamma^{(d)}, \phi^{(d)})} {\log p(w, z, \theta ;{{\alpha^{(t+1)}, \beta^{(t+1)}}}})
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If the convergence standard is satisfied, stop&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence, the Variational EM algorithm can be implemented as function &lt;code&gt;variational_em&lt;/code&gt; below&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def variational_em(self):
    &amp;quot;&amp;quot;&amp;quot;
    Fit LDA model using variational EM
    &amp;quot;&amp;quot;&amp;quot;

    self.init_param()

    old_llhood = -math.inf

    ite = 0

    while True:
        ite += 1
        llhood = self.variational_e_step(self.corpus)
        self.m_step()

        converged = (old_llhood - llhood) / llhood
        old_llhood = llhood

        print(&amp;quot;STEP EM: {} - Likelihood: {} - Converged rate: {}&amp;quot;.\
            format(ite, llhood, converged))

        if converged &amp;lt; cfg.em_threshold:
            break
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;init_param&lt;/code&gt; is to initialize parameter $\alpha, \beta$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; def init_param(self):
    &amp;quot;&amp;quot;&amp;quot;
    Init parameters
    &amp;quot;&amp;quot;&amp;quot;
    self.alpha = np.full(self.K, 1.0)
    self.beta = np.random.randint(1, 50, (self.K, self.V))
    self.beta = self.beta / np.sum(self.beta, axis=1, keepdims=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, two functions &lt;code&gt;variational_e_step&lt;/code&gt; and &lt;code&gt;m_step&lt;/code&gt; are corresponding to &lt;code&gt;E-step&lt;/code&gt; and &lt;code&gt;M-step&lt;/code&gt; of the algorithm, respectively.&lt;/p&gt;
&lt;h3 id=&#34;e-step&#34;&gt;E Step&lt;/h3&gt;
&lt;p&gt;In the &lt;code&gt;E-step&lt;/code&gt;, we perform variational inference for each document $d$ to obtain $\phi^{(d)}, \gamma^{(d)}$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def variational_e_step(self, corpus):
    &amp;quot;&amp;quot;&amp;quot;
    Approximate the posterior distribution

    : corpus - list of documents

    &amp;quot;&amp;quot;&amp;quot;

    total_likelihood = 0

    self.beta_ss = np.zeros((self.K, self.V)) + 1e-20

    for i, doc in enumerate(corpus):
        phi, gamma, doc_likelihood = self.variational_inference(doc)

        # add to total likelihood
        total_likelihood += doc_likelihood

        # update statistics
        for n in range(len(doc)):
            for k in range(self.K):
                self.beta_ss[k, self.word2idx[doc[n]]] += phi[n, k]

    return total_likelihood
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;m-step&#34;&gt;M Step&lt;/h3&gt;
&lt;p&gt;In &lt;code&gt;M-step&lt;/code&gt;, we obtain optimal $\alpha, \beta$. Though, the optimal update for $\alpha$ is kind of complex (Appendix A, section 4.2). Thus, for a simple illustration, we consider &lt;code&gt;alpha&lt;/code&gt; as fixed in the scope of this blog. Setting the derivate of the ELBO $(8)$ with respect to $\beta$ to zero, we yield&lt;/p&gt;
&lt;p&gt;$$
\beta_{ij} \propto \sum_{d=1}^{M} \sum_{n=1}^{N} \phi_{ni}^{(d)} w_{n}^{j} \quad \textrm{(Appendix A, section 4.1)}
$$&lt;/p&gt;
&lt;p&gt;For coding convenience, we implement these updates right in the function &lt;code&gt;variational_e_step&lt;/code&gt; and store these unnormalized results in the variable &lt;code&gt;self.beta_ss&lt;/code&gt;. Hence, in the &lt;code&gt;M-step&lt;/code&gt;, we just normalize and assign it to &lt;code&gt;self.beta&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def m_step(self):
    &amp;quot;&amp;quot;&amp;quot;
    Maximum likelihood estimation
    &amp;quot;&amp;quot;&amp;quot;

    # alpha is considered fixed known constant, hence skip here
    # self.alpha

    # (K, V)
    self.beta = self.beta_ss / np.sum(self.beta_ss, axis=1, keepdims=True)

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;p&gt;Now everything is setup. We then run the following code for training LDA&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = LDA(corpus=cfg.corpus,
            num_topics=10,
            num_words=cfg.max_doc_length,
            num_vocabs=cfg.num_vocabs,
            word2idx=cfg.word2idx,
            idx2word=cfg.idx2word)

model.variational_em()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output will be like&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;STEP EM: 1 - Likelihood: -357373.4968604174 - Converged rate: 1.798192951590305
STEP EM: 2 - Likelihood: -303489.3001228882 - Converged rate: 0.1775489175918577
STEP EM: 3 - Likelihood: -299860.97107707005 - Converged rate: 0.012100037670076039
STEP EM: 4 - Likelihood: -294189.9329730006 - Converged rate: 0.019276791856062188
STEP EM: 5 - Likelihood: -287787.73198189674 - Converged rate: 0.02224626097510863
STEP EM: 6 - Likelihood: -282157.87102739484 - Converged rate: 0.0199528757925569
STEP EM: 7 - Likelihood: -277893.16594093136 - Converged rate: 0.015346563388931922
...........................................
STEP EM: 32 - Likelihood: -264753.70609608945 - Converged rate: 0.00017462683995490507
STEP EM: 33 - Likelihood: -264713.55673188687 - Converged rate: 0.00015167097861647994
STEP EM: 34 - Likelihood: -264676.9793795508 - Converged rate: 0.0001381961983313688
STEP EM: 35 - Likelihood: -264643.03067223117 - Converged rate: 0.00012828113112741676
STEP EM: 36 - Likelihood: -264611.45086573914 - Converged rate: 0.000119344066134337
STEP EM: 37 - Likelihood: -264582.51780543657 - Converged rate: 0.0001093536358432039
STEP EM: 38 - Likelihood: -264556.4361364331 - Converged rate: 9.85864089506385e-05
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;After training, we can extract the top words of the 10 &amp;ldquo;abstract&amp;rdquo; topics.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;topk = 10

indices = np.argpartition(model.beta, -topk, axis=1)[:, -topk:]

topic_top_words_dict = {}
for k in range(model.K):
    topic_top_words_dict[&amp;quot;Topic {}&amp;quot;.format(k + 1)] = \
        [model.idx2word[idx] for idx in indices[k]]

topic_df = pd.DataFrame(topic_top_words_dict)
topic_df
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;0&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Topic 1&lt;/th&gt;
      &lt;th&gt;Topic 2&lt;/th&gt;
      &lt;th&gt;Topic 3&lt;/th&gt;
      &lt;th&gt;Topic 4&lt;/th&gt;
      &lt;th&gt;Topic 5&lt;/th&gt;
      &lt;th&gt;Topic 6&lt;/th&gt;
      &lt;th&gt;Topic 7&lt;/th&gt;
      &lt;th&gt;Topic 8&lt;/th&gt;
      &lt;th&gt;Topic 9&lt;/th&gt;
      &lt;th&gt;Topic 10&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;air&lt;/td&gt;
      &lt;td&gt;murder&lt;/td&gt;
      &lt;td&gt;gets&lt;/td&gt;
      &lt;td&gt;continue&lt;/td&gt;
      &lt;td&gt;denies&lt;/td&gt;
      &lt;td&gt;tas&lt;/td&gt;
      &lt;td&gt;community&lt;/td&gt;
      &lt;td&gt;high&lt;/td&gt;
      &lt;td&gt;act&lt;/td&gt;
      &lt;td&gt;lead&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;takes&lt;/td&gt;
      &lt;td&gt;probe&lt;/td&gt;
      &lt;td&gt;season&lt;/td&gt;
      &lt;td&gt;protesters&lt;/td&gt;
      &lt;td&gt;farmers&lt;/td&gt;
      &lt;td&gt;south&lt;/td&gt;
      &lt;td&gt;public&lt;/td&gt;
      &lt;td&gt;ban&lt;/td&gt;
      &lt;td&gt;three&lt;/td&gt;
      &lt;td&gt;australian&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;baghdad&lt;/td&gt;
      &lt;td&gt;charged&lt;/td&gt;
      &lt;td&gt;trial&lt;/td&gt;
      &lt;td&gt;water&lt;/td&gt;
      &lt;td&gt;vic&lt;/td&gt;
      &lt;td&gt;final&lt;/td&gt;
      &lt;td&gt;go&lt;/td&gt;
      &lt;td&gt;work&lt;/td&gt;
      &lt;td&gt;forces&lt;/td&gt;
      &lt;td&gt;union&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;election&lt;/td&gt;
      &lt;td&gt;aust&lt;/td&gt;
      &lt;td&gt;saddam&lt;/td&gt;
      &lt;td&gt;found&lt;/td&gt;
      &lt;td&gt;korea&lt;/td&gt;
      &lt;td&gt;clash&lt;/td&gt;
      &lt;td&gt;water&lt;/td&gt;
      &lt;td&gt;continues&lt;/td&gt;
      &lt;td&gt;claims&lt;/td&gt;
      &lt;td&gt;four&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;first&lt;/td&gt;
      &lt;td&gt;pm&lt;/td&gt;
      &lt;td&gt;home&lt;/td&gt;
      &lt;td&gt;back&lt;/td&gt;
      &lt;td&gt;boost&lt;/td&gt;
      &lt;td&gt;get&lt;/td&gt;
      &lt;td&gt;missing&lt;/td&gt;
      &lt;td&gt;wins&lt;/td&gt;
      &lt;td&gt;dead&lt;/td&gt;
      &lt;td&gt;minister&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;coast&lt;/td&gt;
      &lt;td&gt;crash&lt;/td&gt;
      &lt;td&gt;top&lt;/td&gt;
      &lt;td&gt;plan&lt;/td&gt;
      &lt;td&gt;urged&lt;/td&gt;
      &lt;td&gt;world&lt;/td&gt;
      &lt;td&gt;new&lt;/td&gt;
      &lt;td&gt;set&lt;/td&gt;
      &lt;td&gt;council&lt;/td&gt;
      &lt;td&gt;mp&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;oil&lt;/td&gt;
      &lt;td&gt;may&lt;/td&gt;
      &lt;td&gt;win&lt;/td&gt;
      &lt;td&gt;anti&lt;/td&gt;
      &lt;td&gt;govt&lt;/td&gt;
      &lt;td&gt;call&lt;/td&gt;
      &lt;td&gt;killed&lt;/td&gt;
      &lt;td&gt;calls&lt;/td&gt;
      &lt;td&gt;support&lt;/td&gt;
      &lt;td&gt;qld&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;coalition&lt;/td&gt;
      &lt;td&gt;iraqi&lt;/td&gt;
      &lt;td&gt;still&lt;/td&gt;
      &lt;td&gt;death&lt;/td&gt;
      &lt;td&gt;iraq&lt;/td&gt;
      &lt;td&gt;cup&lt;/td&gt;
      &lt;td&gt;fire&lt;/td&gt;
      &lt;td&gt;funds&lt;/td&gt;
      &lt;td&gt;us&lt;/td&gt;
      &lt;td&gt;british&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;howard&lt;/td&gt;
      &lt;td&gt;court&lt;/td&gt;
      &lt;td&gt;two&lt;/td&gt;
      &lt;td&gt;sars&lt;/td&gt;
      &lt;td&gt;north&lt;/td&gt;
      &lt;td&gt;australia&lt;/td&gt;
      &lt;td&gt;rain&lt;/td&gt;
      &lt;td&gt;security&lt;/td&gt;
      &lt;td&gt;troops&lt;/td&gt;
      &lt;td&gt;wa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;nsw&lt;/td&gt;
      &lt;td&gt;police&lt;/td&gt;
      &lt;td&gt;woman&lt;/td&gt;
      &lt;td&gt;protest&lt;/td&gt;
      &lt;td&gt;health&lt;/td&gt;
      &lt;td&gt;report&lt;/td&gt;
      &lt;td&gt;hospital&lt;/td&gt;
      &lt;td&gt;drought&lt;/td&gt;
      &lt;td&gt;says&lt;/td&gt;
      &lt;td&gt;group&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Full code available 
&lt;a href=&#34;https://colab.research.google.com/drive/15nqnmXiA3RnfiYPHDrRVMjJVTdszaGJ-?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Latent Dirichlet Allocation. David M. Blei, Andrew Y. Ng, Michael I. Jordan. 2003 (
&lt;a href=&#34;http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Dataset (
&lt;a href=&#34;https://www.kaggle.com/therohk/million-headlines&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Latent Dirichlet Allocation</title>
      <link>https://azraelzhor.github.io/talk/latent-dirichlet-allocation/</link>
      <pubDate>Fri, 03 Apr 2020 16:42:30 +0700</pubDate>
      <guid>https://azraelzhor.github.io/talk/latent-dirichlet-allocation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Variational Expectation Maximization for Latent Dirichlet Allocation - Part 1</title>
      <link>https://azraelzhor.github.io/post/lda-part-1/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://azraelzhor.github.io/post/lda-part-1/</guid>
      <description>&lt;p&gt;Text data is everywhere. When having massive amounts of them, a need naturally arises is that we want them to be organized efficiently. A naive way is to organize them based on topics, meaning that text covering the same topics should be put into the same groups. The problem is that we do not know which topics a text document belongs to and manually labeling topics for all of them is very expensive. Hence, topic modeling comes as an efficient way to automatically discover abstract topics contained in these text documents.&lt;/p&gt;
&lt;img src=&#34;https://i.imgur.com/lQZ2f5r.png&#34; width=&#34;350&#34;/&gt;
&lt;p&gt;One of the most common topic models is Latent Dirichlet Allocation (LDA), was introduced long time ago (D. Blei et al, 2003) but is still powerful now. LDA is a complex, hierarchical latent variable model with some probabilistic assumptions over it. Thus, before diving into detail of LDA, let us review some knowledges about &lt;code&gt;latent variable model&lt;/code&gt; and how to handle some problems associated with it.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; My blog on LDA contains two parts. This is the first part about theoretical understanding of LDA. The  second part involves a basic implementation of LDA, which you can check out 
&lt;a href=&#34;https://azraelzhor.github.io/post/lda-part-2/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;latent-variable-model&#34;&gt;Latent variable model&lt;/h2&gt;
&lt;p&gt;A latent variable model assumes that data, which we can observe, is controlled by some underlying unknown factor. This dependency is often parameterized by a known distribution along with its associated parameters, known as model parameter. A simple latent variable model consists of three parts: observed data $x$, latent variable $z$ that controls $x$ and model parameter $\theta$ like the picture below.
&lt;img src=&#34;https://i.imgur.com/i3cEZQW.png&#34; width=&#34;300&#34;/&gt;&lt;/p&gt;
&lt;!-- [TODO - example of latent variable model]
For an example of latent variable models, imagine that you are an observer at a casino, where people are playing a dice game. The dice dealer rolls the dice at each turn. The dice values range from $1$ to $6$.
$$
1, 1, 2, 3, 1, 1, 1, 3
$$ --&gt;
&lt;p&gt;Latent variables increases our model&amp;rsquo;s expressiveness (meaning our model can capture more complex data) but there&amp;rsquo;s no such thing as a free lunch. Typically, there are two main problems associated with latent variable models that need to be solved&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first one is &lt;strong&gt;learning&lt;/strong&gt; in which we try to find the &amp;ldquo;optimal&amp;rdquo; parameters ${\theta^*}$ based on some criterion. One powerful technique for learning is &lt;code&gt;maximum likelihood estimation&lt;/code&gt; preferring to chose the parameters that maximize the likelihood $p(x;\theta)$. Though, maximum likelihood estimation in latent variable models is hard. A maximum likelihood method named &lt;code&gt;Expectation Maximization&lt;/code&gt; can solve this difficulty for some kind of models. It is also helpful for LDA so we will discuss the method in the next section.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- When $z$ is discrete, we have
$$
p(x; \theta) = \sum_z p(x, z; \theta)
$$ when $z$ is discrete or
$$
p(x; \theta) = \int_z p(x, z; \theta) dz
$$
when $z$ is continuous. --&gt;
&lt;!-- \begin{align}
&amp; \mathbf{E}_{x \sim p_{data}(x)}{p(x; \theta)} \\
&amp; = \frac{1}{N} \prod_i^N p(x^{(i)}; \theta) \\
&amp; = \frac{1}{N} \prod_i^N  \int p(x^{(i)}, z^{(i)}; \theta) dz^{(i)} \\
&amp; = \frac{1}{N} \prod_i^N \int p(x^{(i)} | z^{(i)}; \theta) p(z^{(i)})  dz^{(i)} \\
\end{align} --&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In many cases, latent variables can capture meaningful pattern in the data. Hence, given new data, we are often interested in the value of latent variables. This raises the problem of &lt;strong&gt;inference&lt;/strong&gt; where we want to deduce the posterior $p(x|z;\theta)$. Though, in many cases the posterior is hard to compute. For example, when $z$ is continuous, we have
$$
p(z|x;\theta) = \frac{p(x, z ;\theta)}{p(x;\theta)} = \frac{p(x, z ;\theta)}{\int_z p(x, z;\theta)}
$$&lt;/p&gt;
&lt;p&gt;The integral in the denominator often makes the posterior intractable. A method to solve this problem, named &lt;code&gt;Variational Inference&lt;/code&gt;, will be discussed later.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;expectation-maximization-em&#34;&gt;Expectation Maximization (EM)&lt;/h3&gt;
&lt;p&gt;Introducing latent variables to a statistical model makes its likelihood function non-convex. Thus, it becomes hard to find a maximum likelihood solution. The EM algorithm was introduced to solve the maximum likelihood estimation problem in these kind of statistical models. The algorithm iteratively alternates between building an expected log-likelihood (&lt;code&gt;E step&lt;/code&gt;), which is a convex lower bound to the non-convex log-likelihood, and maximizing it over parameters (&lt;code&gt;M step&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;But how does EM construct the expected log-likelihood?
We have&lt;/p&gt;
&lt;div&gt;
\begin{align}
\log p(x; \theta) &amp; \geq \log p(x;\theta) - KL({{q(z)}}||p(z|x;\theta)) \\
&amp; = \log p(x;\theta) - (\mathrm{E}_{z\sim q(z)}\log q(z) - \mathrm{E}_{z \sim q(z)}\log p(z|x; \theta)) \\
&amp; = \mathrm{E}_{z\sim {q(z)}}(\log p(x;\theta) + \log p(z|x;\theta)) - \mathrm{E}_{z\sim q{(z)}}\log {q(z)} \\
&amp; = \mathrm{E}_{z\sim q(z)}\log p(x, z;\theta) - \mathrm{E}_{z\sim q(z)}\log {q(z)} = L(q, \theta) \tag{1} \\
\end{align}
&lt;/div&gt;
&lt;p&gt;for any choice of $q(z)$. It is obvious that $L(q, \theta)$ is a lower bound of $\log p(x;\theta)$ and the equality holds if and only if $q(z) = p(z|x;\theta)$. EM aims to construct a lower bound that is easy to maximize. By initializing parameter $\theta_{old}$ and choosing $q(z) = p(z|x;\theta_{old})$ at each &lt;code&gt;E-step&lt;/code&gt;, the lower bound becomes&lt;/p&gt;
&lt;div&gt;
$$
L(\theta) = \mathrm{E}_{z\sim p(z|x;\theta_{old})} \log p(x,z;\theta) - \mathrm{E}_{z\sim {p(z|x;\theta_{old})}}\log {p(z|x;\theta_{old})}
$$
&lt;/div&gt;
&lt;p&gt;EM then maximizes $L(\theta)$ at each &lt;code&gt;M-step&lt;/code&gt;&lt;/p&gt;
&lt;div&gt;
\begin{align}
\mathop{max}_{\theta} L(\theta) &amp; = \mathop{max}_{\theta} \mathrm{E}_{z\sim p(z|x;\theta_{old})} \log p(x,z;\theta) - \mathrm{E}_{z\sim {p(z|x;\theta_{old})}}\log {p(z|x;\theta_{old})} \\
&amp; = \mathop{max}_{\theta} \mathrm{E}_{z\sim p(z|x;\theta_{old})} \log p(x,z;\theta) \\
\end{align}
&lt;/div&gt;
&lt;!-- $L(q, \theta)$ is still involved two unknown components which is hard to optimize. EM deals with this problem by initializing parameter $\theta_{old}$ and construct the lower bound by choosing $\color{blue}{q(z)} = p(z|x;\theta_{old})$. The lower bound becomes
$$
L(\theta) = \mathrm{E}_{z\sim p(z|x;\theta_{old})} \log p(x,z;\theta) - \mathrm{E}_{z\sim {p(z|x;\theta_{old})}}\log {p(z|x;\theta_{old})}
$$ --&gt;
&lt;p&gt;The EM algorithm can be summarized as follows&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize parameter $\theta = \theta^{(0)}$&lt;/li&gt;
&lt;li&gt;For each loop $t$ start from $0$
&lt;img src=&#34;https://i.imgur.com/PbhzDrF.png&#34; width=&#34;400&#34;/&gt;
&lt;ul&gt;
&lt;li&gt;Estimate the posterior $p(z|x; {\theta^{(t)}})$&lt;/li&gt;
&lt;li&gt;Maximize the expected log-likelihood
  &lt;div&gt;$\mathop{max}_{{\theta^{(t+1)}}} \mathrm{E}_{z\sim p(z|x ;{\theta^{(t)}})} {p(x, z ;{{\theta^{(t+1)}}}})$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If the convergence standard is satisfied, stop&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: It is easy to notice that the EM algorithm can only be applied if we can compute (or approximate) the posterior distribution analytically, given the current parameter ${\theta^{(t)}}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you want to go into the details of EM, &lt;strong&gt;Gaussian Mixture&lt;/strong&gt; (when $z$ is discrete) and &lt;strong&gt;Probabilistic Principal Component Analysis&lt;/strong&gt; (or Probabilistic PCA in short, when $z$ is continuous) are the two perfect cases to study.&lt;/p&gt;
&lt;h3 id=&#34;variational-inference&#34;&gt;Variational Inference&lt;/h3&gt;
&lt;p&gt;In many of the cases, the posterior distribution $p(z|x;\theta)$ we are interested in can not be inferred analytically, or in other words, it is intractable. This leads naturally to the field of &lt;code&gt;approximate inference&lt;/code&gt;, in which we try to approximate the intractable posterior. Variational inference is such a technique in approximate inference which is fast and effective enough for a good approximation of $p(z|x;\theta)$. The process can be pictured as follows
&lt;img src=&#34;https://i.imgur.com/eqdLdPA.png&#34; width=&#34;400&#34;/&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the idea of variational inference is simple that we reformulate the problem of inference as an optimization problem by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, posit a variational family ${q(z;v)}$ controlled by variational parameter $v$&lt;/li&gt;
&lt;li&gt;Then, find the optimal ${q(z;v^*)}$ in this family, which is as &amp;ldquo;close&amp;rdquo; to $p(z|x;\theta)$ as possible&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Specifically, the goal of variational inference is then to minimize the $KL$ divergence between the variational family and the true posterior: $\mathop{min}_{q, v}KL({{q(z;v)}}||p(z|x;\theta))$. But how can we minimize such an intractable term?&lt;/p&gt;
&lt;!-- But how can we minimize a term that can not be evaluated analytically? --&gt;
&lt;p&gt;Recall from $(1)$ (with the variational distribution ${q(z;v)}$  now being chosen as ${q(z)}$), we have the $ELBO$&lt;/p&gt;
&lt;div&gt;
\begin{align}
&amp; \log p(x;\theta) - KL({{q(z;v)}}||p(z|x;\theta))\\
&amp; = \mathrm{E}_{z\sim {q(z;v)}}\log p(x, z;\theta) - \mathrm{E}_{z\sim {q(z;v)}}\log {q(z;v)}\\
\end{align}
&lt;/div&gt;
&lt;p&gt;Since $\log p(x;\theta)$ is considered as constant, minimizing the KL divergence is equivalent to maximizing the $ELBO$. The optimization problem becomes&lt;/p&gt;
&lt;div&gt;
$$
\mathop{max}_{q}\mathrm{E}_{z\sim {q(z;v)}}\log p(x, z;\theta) - \mathrm{E}_{z\sim {q(z;v)}}\log {{q(z;v)}} \tag{2}
$$
&lt;/div&gt;
&lt;p&gt;which now can be optimized with a suitable choice of ${q(z;v)}$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: EM and variational inference both involve maximizing the ELBO of the log-likelihood. However, EM produces a point estimate of the optimal model parameter, meanwhile variational inference results in an approximation of the posterior distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;mean-field-approximation&#34;&gt;Mean field approximation&lt;/h4&gt;
&lt;p&gt;Several forms of variational inference has been proposed to design a tractable variational family. The simplest out of them is &lt;code&gt;mean-field approximation&lt;/code&gt;, which makes a strong assumption that all latent variables are mutually independent. The variational distribution can then be factorized as
$$
q(z;v) = \prod_{k=1}^{K}q(z_k;v_k) \tag{3}
$$&lt;/p&gt;
&lt;p&gt;where $z$ consists of $K$ latent variables $(z_1, z_2, &amp;hellip;, z_K)$ and each latent variable $z_k$ is controlled by its own variational parameter $v_k$.&lt;/p&gt;
&lt;p&gt;We will not go into detail here, but substituting $(3)$ into $(2)$, taking the derivative with respect to each $q(z_k;v_k)$, then setting the derivative to zero we obtain the coordinate ascent update&lt;/p&gt;
&lt;p&gt;$$
q^*(z_k;v_k^*) \propto \mathrm{E}_{z_{-k} \sim q_{-k}(z_{-k};{v_{-k}})} \log p(z_k, z_{-k}, x;\theta) \tag{4}
$$&lt;/p&gt;
&lt;p&gt;where $(\cdot)_{-k}$ denotes all but the $k$th element.&lt;/p&gt;
&lt;p&gt;Note that until now we didn&amp;rsquo;t specify the functional form for each variational factor $q(z_k;v_k)$ yet. Fortunately, the optimal form of each $q(z_k;v_k)$ can be derived from the $RHS$ expression of the coordinate update $(4)$, which is often easy to work with for many models.&lt;/p&gt;
&lt;!-- Note that in $(3)$ we didn&#39;t specify the functional form for each optimal variational factor $q(z_k;v_k)$. These optimal function forms can be derived from the $RHS$ of the coordinate update, which is easy for many models. --&gt;
&lt;h4 id=&#34;coordinate-ascent-update&#34;&gt;Coordinate ascent update&lt;/h4&gt;
&lt;p&gt;We can then use the coordinate ascent algorithm to find the optimal mean-field distribution. The algorithm can be summarized as follows&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize $v = v^{(0)}$&lt;/li&gt;
&lt;li&gt;For each loop $t$ start from $0$
&lt;ul&gt;
&lt;li&gt;For each loop  $k$ from $1$ to $K$
&lt;ul&gt;
&lt;li&gt;Estimate $q^*(z_k;v_k^*) \propto \mathrm{E}_{z_{-k} \sim q_{-k}(z_{-k};{v_{-k}}^{(t)})} \log p(z_k, z_{-k}, x;\theta)$&lt;/li&gt;
&lt;li&gt;Set $q(z_k; v_k^{(t+1)}) = q^*(z_k;v_k^*)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compute the $ELBO$ to check convergence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By now we are familiar with the concept of latent variable model. Let us move on to discuss LDA in the next section.&lt;/p&gt;
&lt;h2 id=&#34;latent-dirichlet-allocation&#34;&gt;Latent Dirichlet Allocation&lt;/h2&gt;
&lt;p&gt;LDA is a latent variable model on observed text data or, to be more specific, a collection of &lt;code&gt;words&lt;/code&gt; in each &lt;code&gt;document&lt;/code&gt;. The model is built based on the assumptions that each &lt;code&gt;document&lt;/code&gt; is a distribution over a predefined number of &lt;code&gt;topics&lt;/code&gt;; meanwhile, each &lt;code&gt;topic&lt;/code&gt; is considered as a distribution over &lt;code&gt;words&lt;/code&gt; in a fixed vocabulary. For example, suppose that we have $4$ &lt;code&gt;topics&lt;/code&gt; &lt;em&gt;&amp;lt;economics, animal, science, art&amp;gt;&lt;/em&gt; and a total of $6$ &lt;code&gt;words&lt;/code&gt; &lt;em&gt;&amp;lt;money, astronomy, investment, rabbit, painting, chemical&amp;gt;&lt;/em&gt;. Then our assumptions can be illustrated like this figure below.&lt;/p&gt;
&lt;img src=&#34;https://i.imgur.com/tjaOjXs.png&#34; width=&#34;600&#34;/&gt;
&lt;p align=&#34;center&#34;&gt;The two probabilistic assumptions of LDA&lt;/p&gt;
&lt;p&gt;Also, according to this figure, we can say that the document is $40%$ of &lt;em&gt;economics&lt;/em&gt;, $20%$ of &lt;em&gt;animal&lt;/em&gt;, $30%$ of &lt;em&gt;science&lt;/em&gt; and $10%$ of &lt;em&gt;art&lt;/em&gt;. Seem familiar? This is basically the categorical distribution.&lt;/p&gt;
&lt;h3 id=&#34;categorical-distribution&#34;&gt;Categorical distribution&lt;/h3&gt;
&lt;p&gt;Formally, categorical distribution is a &lt;code&gt;discrete&lt;/code&gt; probability distribution, describing the possibility that one discrete random variable belongs to one of $K$ categories. The distribution is parameterized by a $K$-dimensional vector $\theta$ denoting probabilities assigned to each category. It probability mass function is defined as
$$
p(x=i) = \theta_i
$$&lt;/p&gt;
&lt;p&gt;where $x$ is the random variable and $i$ ranges from $1$ to $K$ (representing the $K$ categories).&lt;/p&gt;
&lt;p&gt;In our example above, the document is a categorical distribution over $K = 4$ &lt;code&gt;topics&lt;/code&gt;, with its parameter $\theta = [0.4, 0.2, 0.3, 0.1]$. Similarly, each &lt;code&gt;topic&lt;/code&gt; is also a categorical distribution over $K = 6$ &lt;code&gt;words&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;dirichlet-distribution&#34;&gt;Dirichlet distribution&lt;/h3&gt;
&lt;p&gt;Another distribution which plays an important role in LDA is the Dirichlet distribution (hence the name LDA). Dirichlet distribution is a &lt;code&gt;continuous&lt;/code&gt; multivariate probability distribution over a $(K-1)$-simplex, which can be seen as a set of $K$-dimensional vectors $x=[x_1, x_2, &amp;hellip;, x_K]$ such that each $x_k \geq 0$ and $\sum_{k=1}^Kx_k = 1$. For example, the 2-simplex is a triangle in $3D$ space (see figure below).&lt;/p&gt;
&lt;img src=&#34;https://i.imgur.com/XYL7AoT.png&#34; width=&#34;300&#34;/&gt;
&lt;p&gt;The distribution is parameterized by a positive $K$-dimensional vector $\alpha$, with its probability density function defined as&lt;/p&gt;
&lt;p&gt;$$
p(x;\alpha) = \frac{1}{B(\alpha)} \prod_{k=1}^{K}x_k^{\alpha_k - 1}
$$&lt;/p&gt;
&lt;p&gt;where $B(\cdot)$ is the famous 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;beta function&lt;/a&gt;. The parameter $\alpha$ governs how the density is distributed on the simplex space. For example, the picture below shows how the distribution is concentrated with different $\alpha$ in the case of 2-simplex (brighter color denoting more dense areas).&lt;/p&gt;
&lt;img src=&#34;https://miro.medium.com/max/1400/1*Pepqn_v-WZC9iJXtyA-tQQ.png&#34; width=&#34;500&#34;/&gt;
&lt;p align=&#34;center&#34;&gt;source: &lt;a href=&#34;https://towardsdatascience.com/dirichlet-distribution-a82ab942a879?gi=686effc9deea&#34; target=&#34;_blank&#34;&gt;Dirichlet distribution blog by Sue Liu&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;It is noticeable that sample from a Dirichlet distribution is parameter of a categorical distribution. Thus, Dirichlet distribution is also seen as a distribution over categorical distribution. But why we need the Dirichlet distribution? It is because, in the context of Bayesian statistics, we want to control the uncertainty over some parameters rather than just a point estimate of them. To be more specific, given data $x$ with its likelihood function $f(x;\theta)$, we want to infer the full distribution of $\theta$ given $x$, but not an optimal point $\theta^*$. Given a prior $p(\theta)$, the posterior is proportional to the likelihood time the prior
$$
p(\theta|x) \propto p(x|\theta)p(\theta)
$$&lt;/p&gt;
&lt;p&gt;If the posterior has the same functional form with the prior, the prior is said to be a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Conjugate_prior&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;conjugate prior&lt;/a&gt; to the likelihood. For example, the Dirichlet distribution is the conjugate prior to the categorical likelihood function. It means that, when the likelihood function is categorical and Dirichlet is chosen as a prior, then the posterior
$$
p(\theta|x;\alpha) \propto Cat(x|\theta)Dir(\theta;\alpha)
$$&lt;/p&gt;
&lt;p&gt;will have the same form as the prior, which is a Dirichlet distribution. Conjugate prior makes it easy to calculate the posterior over parameter of interest $\theta$. Thus, in the case of LDA where the categorical distribution is used to represent the topic distribution of each document and the word distribution of each topic, there is no better choice than Dirichlet distribution as a conjugate prior to control these categorical distributions.&lt;/p&gt;
&lt;!-- LDA assumes that each `document` is a distribution over `topics` and each `topic` is considered as a distribution over `words`. For instance, suppose that we have $4$ topics and a total of $6$ `words`.

&lt;img src=&#34;https://i.imgur.com/ek3asj1.png&#34; width=&#34;500&#34;/&gt;
&lt;p align=&#34;center&#34;&gt;Figure _. The two probabilistic assumptions of LDA&lt;/p&gt; --&gt;
&lt;p&gt;LDA is also a generative model. Hence, to understand its architecture clearly, we better see how it generates documents.&lt;/p&gt;
&lt;h3 id=&#34;generative-process&#34;&gt;Generative process&lt;/h3&gt;
&lt;p&gt;Suppose that we have $T$ topics and a vocabulary of $V$ words. Model LDA has 2 parameters $(\alpha, \beta)$ where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\alpha$ denotes the Dirichlet prior that controls topic distribution of each document.&lt;/li&gt;
&lt;li&gt;$\beta$ is a $2D$ matrix of size $T \times V$ denotes word distribution of  all topics ($\beta_i$ is a word distribution of the &lt;code&gt;i + 1&lt;/code&gt;th topic).&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://i.imgur.com/LGAlXEg.png&#34; width=&#34;500&#34;/&gt;
&lt;p align=&#34;center&#34;&gt;How a document is generated in LDA&lt;/p&gt;
&lt;p&gt;The generative process can be pictured as above. Specifically,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For each document $d$ with $N_d$ words
&lt;ul&gt;
&lt;li&gt;Sample document&amp;rsquo;s topic distribution $\theta \sim Dir(\alpha)$&lt;/li&gt;
&lt;li&gt;For each word positions $j$ from $1$ to $N_d$
&lt;ul&gt;
&lt;li&gt;Sample the topic of the current word $t_j \sim Cat(\theta)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sample the current word based on the topic $t_j$ and the word distribution parameters $\beta$, $w_j \sim Cat(\beta_{t_j})$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: $\theta$ is now a latent variable, not model parameter. I keep the notation the same as the original paper for your ease of reference.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;the-two-problems-of-lda&#34;&gt;The two problems of LDA&lt;/h3&gt;
&lt;img src=&#34;https://i.imgur.com/VUMrTKJ.png&#34; width=&#34;400&#34;/&gt;
&lt;p&gt;LDA is a latent variable model, consisting of: observed data $w$;  model parameters $\alpha, \beta$; and latent variables $z, \theta$; as shown in the figure above. Hence, just like any typical latent variable model, LDA also have two problems needed to be solved.&lt;/p&gt;
&lt;!--
$$
p(w;\alpha, \beta) = \int p(\theta;\alpha) \prod_{i=1}^{N_d} \sum_{t=0}^{T - 1} p(z_i = t|\theta) p(w_i | \beta, z_i=t) d\theta
$$ --&gt;
&lt;h4 id=&#34;inference&#34;&gt;Inference&lt;/h4&gt;
&lt;p&gt;Given a document $d$ has $N$ words ${w_1^{(d)}, &amp;hellip;, w_N^{(d)}}$ and model parameters $\alpha$, $\beta$; infer the posterior distribution $p(z, \theta| w^{(d)}; \alpha, \beta)$.&lt;/p&gt;
&lt;p&gt;We can then use mean-field approximation to approximate $p(z, \theta| w^{(d)}; \alpha, \beta)$, by introducing the mean-field variational distribution $q(z, \theta; \gamma, \phi) = q(\theta;\gamma)\prod_{i=1}^{N}q(z_i;\phi_i)$.
&lt;img src=&#34;https://i.imgur.com/Tr25JyX.png&#34; width=&#34;250&#34;/&gt;&lt;/p&gt;
&lt;p&gt;Deriving the ELBO to yield coordinate ascent update for each variational parameter is mathematically heavy so I will not put the mathematical stuff here. For reference, the derivation could be found in the Appendix of the original paper. Based on the coordinate ascent update, we obtain the optimal form for $q(\theta;\gamma)$ which is a Dirichlet distribution and each $q(z_i;\phi_i)$ which is a categorical distribution. The coordinate ascent algorithm then return the optimal parameters $\gamma^*, \phi^*$.&lt;/p&gt;
&lt;h4 id=&#34;parameter-estimation&#34;&gt;Parameter estimation&lt;/h4&gt;
&lt;!-- Likelihood function for each document is --&gt;
&lt;p&gt;In LDA, the problem of parameter estimation is: find $\alpha, \beta$ that maximizes the likelihood function
$$
p(w;\alpha, \beta) = \int p(\theta;\alpha) \prod_{i=1}^{N_d} \sum_{t=0}^{T - 1} p(z_i = t|\theta) p(w_i | \beta, z_i=t) d\theta
$$&lt;/p&gt;
&lt;!-- Given a collection of $D$ documents, find $\alpha, \beta$ that maximizes the likelihood function over all documents

$$
p(D;\alpha, \beta) = \prod_{d}^{D} \int p(\theta_d;\alpha) \prod_{i=1}^{N_d} \sum_{t=0}^{T - 1} p(z_i = t|\theta_d) p(w_i | \beta, z_i=t) d\theta_d
$$ --&gt;
&lt;p&gt;Since the posterior $p(z, \theta| w^{(d)}; \alpha, \beta)$ can not be computed exactly but can only be approximated (for instance, via variational inference in the previous section), we can not apply the EM algorithm directly to solve the estimation problem. To handle this, an algorithm named &lt;strong&gt;variational EM algorithm&lt;/strong&gt;, which combines EM and mean-field inference, was introduced. Variational inference is now used in the E-step to compute the posterior, approximately. The algorithm used for LDA can be summarized as follows&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize parameters $\alpha, \beta$ to $\alpha^{(0)}, \beta^{(0)}$&lt;/li&gt;
&lt;li&gt;For each loop $t$ start from $0$
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;E step&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;For each document $d$
&lt;ul&gt;
&lt;li&gt;Introduce the mean-field $q(z, \theta; \gamma, \phi) = q(\theta;\gamma)\prod_{i=1}^{N}q(z_i;\phi_i)$ to approximate the posterior $p(z^{(d)}, \theta^{(d)}|w^{(d)};{{\alpha^{(t)}, \beta^{(t)}}})$&lt;/li&gt;
&lt;li&gt;Use coordinate ascent update algorithm to yield optimal $\gamma^{(d)}, \phi^{(d)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;M step&lt;/strong&gt;: Maximize the expected log-likelihood (up to some constant) with respect to $\alpha, \beta$
  &lt;div&gt;$$
  \mathop{max}_{{\alpha^{(t+1)}, \beta^{(t+1)}}} \sum_{d=1}^{M} \mathrm{E}_{z, \theta \sim q(z, \theta; \gamma^{(d)}, \phi^{(d)})} {\log p(w, z, \theta ;{{\alpha^{(t+1)}, \beta^{(t+1)}}}})
  $$&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;If the convergence standard is satisfied, stop&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Actually, there are many techniques to solve the two problems of LDA. Though, we only discuss about Variational EM in the scope of this blog :v&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Latent Dirichlet Allocation (
&lt;a href=&#34;http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Mean-field variational inference(
&lt;a href=&#34;https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture13.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
