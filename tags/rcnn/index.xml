<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rcnn | Thang Le</title>
    <link>https://azraelzhor.github.io/tags/rcnn/</link>
      <atom:link href="https://azraelzhor.github.io/tags/rcnn/index.xml" rel="self" type="application/rss+xml" />
    <description>rcnn</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>joocxi@2020</copyright><lastBuildDate>Sat, 15 Feb 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=true) shape:circle]</url>
      <title>rcnn</title>
      <link>https://azraelzhor.github.io/tags/rcnn/</link>
    </image>
    
    <item>
      <title>Dive into Faster RCNN</title>
      <link>https://azraelzhor.github.io/post/faster-rcnn/</link>
      <pubDate>Sat, 15 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://azraelzhor.github.io/post/faster-rcnn/</guid>
      <description>&lt;p&gt;Last year, I had a chance to be involved in an Advanced Computer Vision class held by a non-profit organization. During the class, object detection is one of the fields that I found myself interested in the most. This motivated me to write a series of blogs in order to understand better some famous approaches that has been applied in the field. Though, the idea has been postponed until recently :v. The first part of this series is about Faster RCN, one of the state-of-the-art method used for object detection. In this blog post, I will walk you through the detail of Faster RCNN. Hopefully, at the end of this blog, you would figure out the way Faster RCNN works.&lt;/p&gt;
&lt;h2 id=&#34;outline&#34;&gt;Outline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#intro&#34;&gt;Object detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#faster-rcnn&#34;&gt;Faster RCNN&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;&#34;&gt;VGG Shared Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;&#34;&gt;Region Proposal Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;&#34;&gt;Region-based CNN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#loss-function&#34;&gt;Loss Function&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;&#34;&gt;Regression Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;&#34;&gt;Classification Loss&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#training&#34;&gt;Training&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;&#34;&gt;Augment data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;&#34;&gt;Create anchor generator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#detection&#34;&gt;Detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;a-little-bit-of-object-detection&#34;&gt;A little bit of object detection&lt;/h2&gt;
&lt;p&gt;In object detection, we received an image with bounding boxes indicating a various type of objects. There are many approaches these days.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/agk4axh.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id=&#34;faster-rcnn&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;faster-rcnn-architecture&#34;&gt;Faster RCNN architecture&lt;/h2&gt;
&lt;p&gt;Faster RCNN is the last detection model in the RCNN trilogy (RCNN - Fast RCNN - Faster RCNN), which relies on proposed regions to detect objects. Though, unlike its predecessors  which use selective search to find out the best regions, Faster RCNN makes use of neural network and &amp;ldquo;learn&amp;rdquo; to propose regions directly. These proposed regions then is fed into another neural network to be refined once again.&lt;/p&gt;
&lt;!-- This is the main reason that makes Faster RCNN faster and better than its predecessors. --&gt;
&lt;p&gt;First, let take a look at the overall architecture of Faster RCNN. It comprises of $2$ modules&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The region proposal module takes feature map from a feature network and proposes regions&lt;/li&gt;
&lt;li&gt;The Fast RCNN detector module takes those regions to predict the classes that the object belongs to.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Zsu3nEn.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The feature network, which is VGG in the context of this blog, is shared between both model.&lt;/p&gt;
&lt;p&gt;To easily keep track of the story, let&amp;rsquo;s follow a specific example in which we are given an image of shape $300\times400\times3$.&lt;/p&gt;
&lt;h3 id=&#34;feature-shared-network&#34;&gt;Feature Shared Network&lt;/h3&gt;
&lt;p&gt;[TODO]Pretrained models&lt;/p&gt;
&lt;p&gt;We use VGG as a feature network. The VGG receives an input image and produce a feature map with reduced sizes. The size of the feature map is determined by the net structure. For example, in case we use VGG, the feature map&amp;rsquo;s shape is $18 \times 25 \times 512$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/dut8uoM.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;region-proposal-network-rpn&#34;&gt;Region Proposal Network (RPN)&lt;/h3&gt;
&lt;p&gt;The goal of RPN is to propose regions that highly contain object. In order to do that, RPN does&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;generate a predefined number of fixed-size anchors&lt;/li&gt;
&lt;li&gt;predict the objectness of each of these anchors&lt;/li&gt;
&lt;li&gt;refine their coordinates&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;predefined-anchors&#34;&gt;Predefined anchors&lt;/h4&gt;
&lt;!-- RPN accepts VGG feature map as input. --&gt;
&lt;p&gt;For each pixel spatial location on the VGG feature map, we generate a predefined number of fixed size anchors. The shape of these anchor boxes are determined by a combination of predefined scales and edge ratios. In our example, if we use $3$ scales $64$, $128$, $256$ and $3$ edge ratios $1:1$, $1:2$, $2:1$, there will be $3*3=9$ type of anchors at each pixel location and a total of $18 * 25 * 9 = 4050$ anchors to be generated as a result.
&lt;img src=&#34;https://i.imgur.com/BxG5M0Z.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;It is important to note that even though anchor boxes are created based on the feature map&amp;rsquo;s spatial location, they reference to the original input image, in which anchor boxes generated from the same feature map pixel location are centered at the same point on the original input, as illustrated in this figure below.
&lt;img src=&#34;https://i.imgur.com/BNTidcL.png&#34; width=&#34;400&#34;/&gt;&lt;/p&gt;
&lt;!-- ![](https://i.imgur.com/3D1N77A.png) --&gt;
&lt;!-- ![](https://i.imgur.com/scAnbm9.png) --&gt;
&lt;h4 id=&#34;rpn-architecture&#34;&gt;RPN architecture&lt;/h4&gt;
&lt;!-- It consists of 3 convolution layers: one convolutional layer with 512 filters of size 3x3 followed by two sibling 1x1 convolutional layers - one with $K$ filters acting as a classifier and the other with $4K$ filters acting as a regressor. --&gt;
&lt;p&gt;The RPN is then designed to predict objectness of each anchor (classification) and its coordinates (regression). It consists of $3$ layers: one convolutional layer with $512$ filters of size $3 \times 3$ followed by two sibling $1 \times 1$ convolutional layers. These two sibling layers - one with $K$ filters and the other with $4K$ filters - allow for classification and regression, respectively.&lt;/p&gt;
&lt;p&gt;&lt;img                    src=&#34;https://i.imgur.com/o1pTYG2.png&#34; width=&#34;500&#34;
/&gt;&lt;/p&gt;
&lt;p&gt;In our example, after passing the VGG feature map through RPN, it produces a classification output with shape of $18 \times 25 \times K$ and a regression output with shape of $18 \times 25 \times 4K$, where $K$ denotes the number of generated anchors at each feature map location.&lt;/p&gt;
&lt;!-- ![](https://i.imgur.com/o1pTYG2.png) --&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def rpn(base_layers, num_anchors):
    x = Conv2D(512, (3, 3), padding=&#39;same&#39;, activation=&#39;relu&#39;, kernel_initializer=&#39;normal&#39;, name=&#39;rpn_conv1&#39;)(base_layers)

    x_class = Conv2D(num_anchors, (1, 1), activation=&#39;sigmoid&#39;, kernel_initializer=&#39;uniform&#39;, name=&#39;rpn_out_class&#39;)(x)
    x_regr = Conv2D(num_anchors * 4, (1, 1), activation=&#39;linear&#39;, kernel_initializer=&#39;zero&#39;, name=&#39;rpn_out_regress&#39;)(x)

    return [x_class, x_regr, base_layers]
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;create-labeled-data-for-training-rpn&#34;&gt;Create labeled data for training RPN&lt;/h4&gt;
&lt;p&gt;Now, we need labeled data to train the RPN.&lt;/p&gt;
&lt;h5 id=&#34;label-for-classification&#34;&gt;Label for classification&lt;/h5&gt;
&lt;p&gt;For training classification task, each anchor box is labeled as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;positive - containing object&lt;/li&gt;
&lt;li&gt;negative - background&lt;/li&gt;
&lt;li&gt;ignored - being ignored when training&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;based on the overlap with its nearest ground-truth bounding box.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/3m5ITZD.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We use IoU to measure these overlaps. Let $p$ denotes the IoU between current anchor box and its nearest ground-truth bounding box. The rule is detailed as follows&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If $p &amp;gt; \text{max_rpn}$, label it positive&lt;/li&gt;
&lt;li&gt;If $p &amp;lt; \text{min_rpn}$, label it negative&lt;/li&gt;
&lt;li&gt;If $\text{min_rpn} &amp;lt; p &amp;lt; \text{max_rpn}$, ignore it when training&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# overlaps for RPN
cfg.rpn_min_overlap = 0.3
cfg.rpn_max_overlap = 0.7
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;label-for-regression&#34;&gt;Label for regression&lt;/h5&gt;
&lt;p&gt;The anchor box refinement is modeled as a regression problem, in which we predict the delta $(\color{red}{t_x, t_y, t_w, t_h})$ for each anchor box. This delta denotes the change needed to refine our predefined anchor boxes, as illustrated in this figure below&lt;/p&gt;
&lt;!-- ![](https://i.imgur.com/x7kGAvI.png) --&gt;
&lt;img src=&#34;https://i.imgur.com/7h3T6TK.png&#34; width=&#34;400&#34;/&gt;
&lt;p&gt;Formally, we have&lt;/p&gt;
&lt;div&gt;
$$\begin{align}
\color{blue}{x} &amp; = x_a + \color{red}{t_x}*w_a \\
\color{blue}{y} &amp; = y_a + \color{red}{t_y}*h_a \\
\color{blue}{w} &amp; = w_a * e^{\color{red}{t_w}} \\
\color{blue}{h} &amp; = h_a * e^{\color{red}{t_h}}
\end{align}
$$
&lt;/div&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;div&gt;
$$\begin{align}
\color{red}{t_x} &amp; = (\color{blue}{x} - x_a) / w_a \\
\color{red}{t_y} &amp; = (\color{blue}{y} - y_a) / h_a \\
\color{red}{t_w} &amp; = log(\color{blue}{w}/w_a) \\
\color{red}{t_h} &amp; = log(\color{blue}{h}/h_a)
\end{align}
$$
&lt;/div&gt;
&lt;p&gt;where $(x_a, y_a, w_a, h_a)$ denotes the anchor box&amp;rsquo;s coordinates and $(\color{blue}{x, y, w, h})$ denotes the refined box&amp;rsquo;s coordinates.&lt;/p&gt;
&lt;p&gt;To create data for anchor regression training, we calculate the &amp;ldquo;ground-truth&amp;rdquo; delta $(\color{red}{t_x^*, t_y^*, t_w^*, t_h^*})$ based on each anchor box&amp;rsquo;s coordinates $(x_a, y_a, w_a, h_a)$ and its nearest ground-truth bounding box&amp;rsquo;s coordinates $(\color{blue}{x^*, y^*, w^*, h^*})$.&lt;/p&gt;
&lt;div&gt;
$$\begin{align}
\color{red}{t_x^*} &amp; = (\color{blue}{x^*} - x_a) / w_a \\
\color{red}{t_y^*} &amp; = (\color{blue}{y^*} - y_a) / h_a \\
\color{red}{t_w^*} &amp; = log(\color{blue}{w^*}/w_a) \\
\color{red}{t_h^*} &amp; = log(\color{blue}{h^*}/h_a)
\end{align}
$$
&lt;/div&gt;
&lt;p&gt;Among those generated anchor boxes, the positive anchors are probably outnumbered by the negative ones. Thus, to avoid imbalanced classification, we only use some anchor boxes for training. Specifically, only $256$ anchor boxes is chosen for training the RPN.&lt;/p&gt;
&lt;p&gt;For example, with $4050$ anchor boxes generated, assume that we have $4000$ anchor boxes labeled as &amp;ldquo;positive&amp;rdquo;, $50$ anchor boxes labeled as &amp;ldquo;negative&amp;rdquo;&lt;/p&gt;
&lt;h4 id=&#34;rpn-losses&#34;&gt;RPN losses&lt;/h4&gt;
&lt;h5 id=&#34;1-regression-loss&#34;&gt;1. Regression Loss&lt;/h5&gt;
&lt;p&gt;The smooth L1 loss is used for regression training. Its formulation is as below&lt;/p&gt;
&lt;p&gt;$$smooth_{L1}(x) =
\begin{cases}
0.5x^2 &amp;amp; \mbox{if} ;  \lvert x \rvert &amp;lt; 1, \&lt;br&gt;
\lvert x \rvert - 0.5 &amp;amp; \mbox{otherwise}.
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;where $x$ denotes the difference between prediction and ground truth $t  - \color{blue}{t^*}$.&lt;/p&gt;
&lt;img src=&#34;https://i.imgur.com/HKcpwC2.png&#34; width=&#34;300&#34;/&gt;
&lt;p&gt;The reason smooth L1 loss is preferred to L1 and L2 loss is because it can handle the problem of these two losses. Being quadratic for small values ($\lvert x \rvert &amp;lt; 1$) and linear for large values ($\lvert x \rvert \geq 1$), smooth L1 loss is now less sensitive to outliers than L2 loss and also does not suffer from the problem of L1 loss, which is not differentiable around zero.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# regression loss for rpn
def rpn_loss_regr(cfg):
    def rpn_loss_regr_fixed_num(y_true, y_pred):
        x = y_true[:, :, :, 4 * cfg.num_anchors:] - y_pred
        x_abs = K.abs(x)
        x_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)

        return cfg.lambda_rpn_regr * K.sum(
            y_true[:, :, :, :4 * cfg.num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(cfg.epsilon + y_true[:, :, :, :4 * cfg.num_anchors])

    return rpn_loss_regr_fixed_num
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;2-classification-loss&#34;&gt;2. Classification Loss&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def rpn_loss_cls(cfg):
    def rpn_loss_cls_fixed_num(y_true, y_pred):
        return cfg.lambda_rpn_class * K.sum(y_true[:, :, :, :cfg.num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, cfg.num_anchors:])) / K.sum(cfg.epsilon + y_true[:, :, :, :cfg.num_anchors])

    return rpn_loss_cls_fixed_num
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;use-rpn-to-propose-regions&#34;&gt;Use RPN to propose regions&lt;/h4&gt;
&lt;h5 id=&#34;rpn-prediction&#34;&gt;RPN prediction&lt;/h5&gt;
&lt;img src=&#34;https://i.imgur.com/uNnQDrw.png&#34; width=&#34;400&#34;/&gt;
&lt;p&gt;After training, we use RPN to predict the bounding box coordinates at each feature map location.&lt;/p&gt;
&lt;!-- $$\begin{align}
\color{blue}{x} &amp; = x_a + \color{red}{t_x}*w_a \\
\color{blue}{y} &amp; = y_a + \color{red}{t_y}*h_a \\
\color{blue}{w} &amp; = w_a * e^{\color{red}{t_w}} \\
\color{blue}{h} &amp; = h_a * e^\color{red}{t_h}
\end{align}$$ --&gt;
&lt;p&gt;In our example, there is $4050$ anchor boxes in total. Assume that the RPN predict $3000$ positive bounding boxes - meaning that they are containing object.&lt;/p&gt;
&lt;h5 id=&#34;non-max-suppression&#34;&gt;Non-max suppression&lt;/h5&gt;
&lt;img src=&#34;https://i.imgur.com/dn7grUV.png&#34; width=&#34;400&#34;/&gt;
&lt;p&gt;It is very likely that there are many bounding boxes, among those are predicted by RPN, referring to the same object. This leads to redundant proposals, which can be eliminated by an algorithm known as non max suppression. The idea of non max suppression is to filter out all but the box with highest confidence score for each highly-overlapped bounding box cluster, making sure that a particular object is identified only once.&lt;/p&gt;
&lt;p&gt;The algorithm can be summarized as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Given a list of proposals along with their confidence score, and a predefined overlap threshold
&lt;ul&gt;
&lt;li&gt;Initialize a list $L$ to contain bounding boxes.&lt;/li&gt;
&lt;li&gt;Sort the list, denoted by $S$,  by confidence score in descending order&lt;/li&gt;
&lt;li&gt;Iterate through $S$, at each iteration
&lt;ul&gt;
&lt;li&gt;Compute the overlap between the current bounding box and the remain bounding boxes in $S$&lt;/li&gt;
&lt;li&gt;Suppress all bounding boxes that have the computed overlap above the predefined threshold hold from $S$&lt;/li&gt;
&lt;li&gt;Discard the current box from $S$, then move it to $L$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Return $L$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&#34;https://i.imgur.com/Mh1L9XC.png&#34; width=&#34;400&#34;/&gt;
&lt;h3 id=&#34;region-based-convolutional-neural-network&#34;&gt;Region-based Convolutional Neural Network&lt;/h3&gt;
&lt;p&gt;Now we have feature map patches as regions ready for the next phase. Now one notable problem arises here is that those proposed regions are not in the same shape, which makes it difficult for neural network training. This is where we need RoI pooling layer to help construct fixed-size feature maps from those arbitrary-size regions.&lt;/p&gt;
&lt;h4 id=&#34;roi-pooling&#34;&gt;RoI Pooling&lt;/h4&gt;
&lt;p&gt;To understand RoI pooling, let begin with a 2D example.&lt;/p&gt;
&lt;h5 id=&#34;a-2d-example&#34;&gt;A 2D example&lt;/h5&gt;
&lt;!--  Given an input slice of arbitrary size, --&gt;
&lt;p&gt;No matter what the shape of the input slice is, a $2 \times 2$ RoI pooling layer always transform the input to the output of size $2 \times 2$ by&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Split the input into a $2 \times 2$ matrix of roughly equal regions&lt;/li&gt;
&lt;li&gt;Do max pooling on each region&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;like this figure below (given input of shape $4 \times 4$ or $5 \times 5$).&lt;/p&gt;
&lt;!-- ![](https://i.imgur.com/oSbRTQf.png) --&gt;
&lt;img src=&#34;https://i.imgur.com/0Z6wlit.png&#34; width=&#34;400&#34;/&gt;
&lt;h5 id=&#34;roi-used-in-faster-rcnn&#34;&gt;RoI used in Faster RCNN&lt;/h5&gt;
&lt;img src=&#34;https://i.imgur.com/Clu7DyN.png&#34; width=&#34;400&#34;/&gt;
&lt;h4 id=&#34;detection-network&#34;&gt;Detection Network&lt;/h4&gt;
&lt;!-- Those fixed-size feature maps from RoI pooling are subsequently fed into the final classifier. --&gt;
&lt;p&gt;Those fixed-size feature maps from RoI pooling are then flattened and subsequently fed into a fully connected network for final detection. The net consists of $2$ fully connected layers of $4096$ neurons, followed by other $2$ sibling fully connected layers - one has $N$ neurons for classifying proposals and the other has $4*(N - 1)$ neurons for bounding box regression, where $N$ denotes the number of classes, including the background. Note that when a bounding box is classified as background, regression is unneeded. Hence, it makes sense that we only need $4*(N - 1)$ neurons for regression in total.&lt;/p&gt;
&lt;img src=&#34;https://i.imgur.com/o05O9LM.png&#34; width=&#34;500&#34;/&gt;
&lt;p&gt;In our example, each $7\times7\times512$ feature map is fed to the detection net to produce the classification output has size of $4$, and the regression output has size of $12$.&lt;/p&gt;
&lt;h4 id=&#34;labeled-data-for-fcnn&#34;&gt;Labeled data for FCNN&lt;/h4&gt;
&lt;!-- After non max suppresion step, we get
For each proposed region, the RPN predict  --&gt;
&lt;h5 id=&#34;label-for-classification-1&#34;&gt;Label for classification&lt;/h5&gt;
&lt;p&gt;Similar to the RPN, we make use of IoU metric to label data. Let $p$ denotes the overlap between a refined anchor box produced by RPN and its nearest ground-truth anchor box. For each anchor box we label as follows&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if $p &amp;lt; \text{min_cls}$, ignore it when training.&lt;/li&gt;
&lt;li&gt;if $\text{min_cls} \leq p &amp;lt; \text{max_cls}$, label it as background.&lt;/li&gt;
&lt;li&gt;if $p \geq \text{max_cls}$, label it as the class to which its nearest ground-truth box belongs.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cfg.classifier_min_overlap = 0.1
cfg.classifier_max_overlap = 0.5
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;label-for-bounding-box-regression&#34;&gt;Label for bounding box regression&lt;/h5&gt;
&lt;p&gt;For regression, we also calculate the &amp;ldquo;ground-truth&amp;rdquo; deltas $(\color{red}{t_x^*, t_y^*, t_w^*, t_h^*})$ in the same fashionÂ as those in RPN, but now based on each refined anchor box&amp;rsquo;s coordinates from the RPN $(x_r, y_r, w_r, h_r)$ and its nearest ground-truth bounding box&amp;rsquo;s coordinates $(\color{blue}{x^*, y^*, w^*, h^*})$.&lt;/p&gt;
&lt;!-- &lt;a id=&#34;loss-function&#34;&gt;&lt;/a&gt; --&gt;
&lt;h4 id=&#34;rcnn-losses&#34;&gt;RCNN losses&lt;/h4&gt;
&lt;h5 id=&#34;1-regression-loss-1&#34;&gt;1. Regression Loss&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# regresssion loss for detection network
def class_loss_regr(num_classes, cfg):
    def class_loss_regr_fixed_num(y_true, y_pred):
        x = y_true[:, :, 4*num_classes:] - y_pred
        x_abs = K.abs(x)
        x_bool = K.cast(K.less_equal(x_abs, 1.0), &#39;float32&#39;)
        return cfg.lambda_cls_regr * K.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(cfg.epsilon + y_true[:, :, :4*num_classes])
    return class_loss_regr_fixed_num
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id=&#34;2-classification-loss-1&#34;&gt;2. Classification Loss&lt;/h5&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def class_loss_cls(cfg):
    def class_loss_cls_fixed_num(y_true, y_pred):
        return cfg.lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))
    return class_loss_cls_fixed_num
&lt;/code&gt;&lt;/pre&gt;
&lt;!-- &lt;a id=&#34;training&#34;&gt;&lt;/a&gt;
## Training

&lt;a id=&#34;detection&#34;&gt;&lt;/a&gt;
## Detection --&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (
&lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arxiv&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
