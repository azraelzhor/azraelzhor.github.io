<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Thang Le">

  
  
  
    
  
  <meta name="description" content="After discussing LDA in the previous part, we now get our hands dirty by implementing the Variational EM algorithm. Recall that the goal is to estimate $\alpha, \beta$ that maximizes the ELBO
 $$ \mathrm{E}_{q}\log p(w, \theta, z;\alpha, \beta) - \mathrm{E}_{q}\log {q(z, \theta;\gamma, \phi)} $$  where the joint likelihood and the variational distribution are factorized as follows
$$ p(w, \theta, z; \alpha, \beta) = Dir(\theta;\alpha)\prod_{n=1}^{N}Cat(z_n;\theta) Cat(w_n;z_n, \beta) $$
$$ q(z, \theta; \gamma, \phi) = Dir(\theta;\gamma)\prod_{n=1}^{N}Cat(z_n;\phi_n) $$">

  
  <link rel="alternate" hreflang="en-us" href="https://azraelzhor.github.io/post/lda-part-2/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/atelier-sulphurpool-light.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/atelier-sulphurpool-light.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://azraelzhor.github.io/post/lda-part-2/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@joocxi">
  <meta property="twitter:creator" content="@joocxi">
  
  <meta property="og:site_name" content="Thang Le">
  <meta property="og:url" content="https://azraelzhor.github.io/post/lda-part-2/">
  <meta property="og:title" content="Variational Expectation Maximization for Latent Dirichlet Allocation - Part 2 | Thang Le">
  <meta property="og:description" content="After discussing LDA in the previous part, we now get our hands dirty by implementing the Variational EM algorithm. Recall that the goal is to estimate $\alpha, \beta$ that maximizes the ELBO
 $$ \mathrm{E}_{q}\log p(w, \theta, z;\alpha, \beta) - \mathrm{E}_{q}\log {q(z, \theta;\gamma, \phi)} $$  where the joint likelihood and the variational distribution are factorized as follows
$$ p(w, \theta, z; \alpha, \beta) = Dir(\theta;\alpha)\prod_{n=1}^{N}Cat(z_n;\theta) Cat(w_n;z_n, \beta) $$
$$ q(z, \theta; \gamma, \phi) = Dir(\theta;\gamma)\prod_{n=1}^{N}Cat(z_n;\phi_n) $$"><meta property="og:image" content="img/map[gravatar:%!s(bool=true) shape:circle]">
  <meta property="twitter:image" content="img/map[gravatar:%!s(bool=true) shape:circle]"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-05-12T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-05-12T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://azraelzhor.github.io/post/lda-part-2/"
  },
  "headline": "Variational Expectation Maximization for Latent Dirichlet Allocation - Part 2",
  
  "datePublished": "2020-05-12T00:00:00Z",
  "dateModified": "2020-05-12T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Thang Le"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Thang Le",
    "logo": {
      "@type": "ImageObject",
      "url": "https://azraelzhor.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "After discussing LDA in the previous part, we now get our hands dirty by implementing the Variational EM algorithm. Recall that the goal is to estimate $\\alpha, \\beta$ that maximizes the ELBO\n $$ \\mathrm{E}_{q}\\log p(w, \\theta, z;\\alpha, \\beta) - \\mathrm{E}_{q}\\log {q(z, \\theta;\\gamma, \\phi)} $$  where the joint likelihood and the variational distribution are factorized as follows\n$$ p(w, \\theta, z; \\alpha, \\beta) = Dir(\\theta;\\alpha)\\prod_{n=1}^{N}Cat(z_n;\\theta) Cat(w_n;z_n, \\beta) $$\n$$ q(z, \\theta; \\gamma, \\phi) = Dir(\\theta;\\gamma)\\prod_{n=1}^{N}Cat(z_n;\\phi_n) $$"
}
</script>

  

  


  


  





  <title>Variational Expectation Maximization for Latent Dirichlet Allocation - Part 2 | Thang Le</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Thang Le</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Thang Le</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Blogs</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      

      

    </ul>

  </div>
</nav>


  
<div class="container-fluid docs">
  <div class="row flex-xl-nowrap">
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      <ul class="nav toc-top">
        <li>
          <a href="#" id="back_to_top" class="docs-toc-title">Contents</a>
        </li>
      </ul>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#working-with-the-elbo">Working with the ELBO</a>
      <ul>
        <li><a href="#dealing-with-expected-values">Dealing with expected values</a></li>
        <li><a href="#deriving-the-elbo">Deriving the ELBO</a></li>
      </ul>
    </li>
    <li><a href="#preparing-data">Preparing data</a></li>
    <li><a href="#preprocessing-data">Preprocessing data</a></li>
    <li><a href="#global-configuration">Global configuration</a></li>
    <li><a href="#lda-model-definition">LDA model definition</a></li>
    <li><a href="#compute-the-log-likelihood">Compute the log-likelihood</a></li>
    <li><a href="#variational-inference">Variational Inference</a></li>
    <li><a href="#variational-em">Variational EM</a>
      <ul>
        <li><a href="#e-step">E Step</a></li>
        <li><a href="#m-step">M Step</a></li>
      </ul>
    </li>
    <li><a href="#training">Training</a></li>
    <li><a href="#result">Result</a></li>
  </ul>
</nav>
      
    </div>

    <main class="col-12 col-md-0 col-xl-10 py-md-3 pl-md-5 docs-content" role="main">

      <article class="article">
        












  

  
  
  
<div class="article-container pt-3">
  <h1>Variational Expectation Maximization for Latent Dirichlet Allocation - Part 2</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    May 12, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/post/lda-part-2/#disqus_thread"></a>
  

  
  

</div>

    














  
</div>


        <div class="article-container">
          <div class="article-style">
            <p>After discussing LDA in the previous part, we now get our hands dirty by implementing the Variational EM algorithm. Recall that the goal is to estimate $\alpha, \beta$ that maximizes the ELBO</p>
<div>
$$
\mathrm{E}_{q}\log p(w, \theta, z;\alpha, \beta) - \mathrm{E}_{q}\log {q(z, \theta;\gamma, \phi)}
$$
</div>
<p>where the joint likelihood and the variational distribution are factorized as follows</p>
<p>$$
p(w, \theta, z; \alpha, \beta) = Dir(\theta;\alpha)\prod_{n=1}^{N}Cat(z_n;\theta) Cat(w_n;z_n, \beta)
$$</p>
<p>$$
q(z, \theta; \gamma, \phi) = Dir(\theta;\gamma)\prod_{n=1}^{N}Cat(z_n;\phi_n)
$$</p>
<h2 id="working-with-the-elbo">Working with the ELBO</h2>
<p>But before getting into code, we need to derive the ELBO. Substituting these factorizations into the ELBO, we obtain</p>
<div>
\begin{align}
L & = \mathrm{E}_{q}\log p(w, \theta, z;\alpha, \beta) - \mathrm{E}_{q}\log {q(z, \theta;\gamma, \phi)} \\
& = \mathrm{E}_{q}\log Dir(\theta; \alpha) + \sum_{n=1}^{N} \Big[ \mathrm{E}_{q} \log Cat(z_n; \theta) + \mathrm{E}_{q} \log Cat(w_n; z_n, \beta) \Big] \\
& \quad - \mathrm{E}_{q} \log Dir(\theta; \gamma) - \sum_{n=1}^{N}\mathrm{E}_{q} \log Cat (z_n;\phi_n) \tag{1}
\end{align}
</div>
<h3 id="dealing-with-expected-values">Dealing with expected values</h3>
<p>To handle the expectations in the ELBO, we need to rewrite the Dirichlet distribution in exponential form as follows</p>
<div>
\begin{align}
Dir(x;\alpha) & = \frac{1}{B(\alpha)} \prod_{k=1}^{K}x_k^{\alpha_k - 1} \\
& = \frac{\Gamma(\sum_{k=1}^{K}\alpha_k)}{\sum_{k=1}^{K}\Gamma(\alpha_k)} \prod_{k=1}^{K}x_k^{\alpha_k - 1} \\
& = \exp\Big[ \sum_{k=1}^{K} (\alpha_k - 1) \log x_k + \log \Gamma(\sum_{k=1}^{K}\alpha_k) - \sum_{k=1}^{K}\log\Gamma(\alpha_k) \Big]
\end{align}
</div>
<blockquote>
<p><strong>Exponential family distribution</strong>
$$
p(x|\theta) = h(x) exp(\eta \cdot T(x) - A(\eta))
$$</p>
<p>where $h(x)$ is known as <strong>base measure</strong>, $\eta(\theta)$ is <strong>natural parameter</strong>, $T(x)$ is <strong>sufficient statistic</strong> and $A(\theta)$ is <strong>log normalizer</strong>. One important property of the exponential family is that the mean of the sufficient statistic $T(x)$ can be derived by differentiating the natural parameter $A(\eta)$
$$
E[T_j]= \frac{\partial A(\eta)}{\partial \eta_j} \tag{2}
$$</p>
</blockquote>
<p>Applying property $(2)$ for the case of the Dirichlet distribution, we have</p>
<div>
\begin{align}
\mathrm{E}_{x \sim Dir(x;\alpha)}\log x_k & = \frac{\partial(\sum_{j=1}^{K}\log\Gamma(\alpha_j) - \log \Gamma(\sum_{j=1}^{K}\alpha_j))  }{\partial (\alpha_k - 1)} \\
& = \frac{\partial(\sum_{j=1}^{K}\log\Gamma(\alpha_j) - \log \Gamma(\sum_{j=1}^{K}\alpha_j))  }{\partial \alpha_k} \\
& = \Psi(\alpha_k) - \Psi(\sum_{j=1}^{K}\alpha_j)
\end{align}
</div>
<p>where $\Psi(\cdot)$ is the derivative of the logarithm of Gamma function (also known as Digamma).</p>
<p>Also, the categorical distribution can be represented using 
<a href="https://en.wikipedia.org/wiki/Iverson_bracket" target="_blank" rel="noopener">Iverson bracket</a> $[\cdot]$</p>
<p>$$
Cat(x;\theta) = \prod_{i=1}^K \theta_i^{[x=i]}
$$</p>
<p>where $[x=i]$ evaluates to  $1$ if $x = i$, $0$ otherwise (with the assumption that values of $x$ fall into the range ${1, 2, &hellip;, K}$).</p>
<!-- $$
\log Cat(x;\theta) = \sum_{i=1}^K [x=i] \log \theta_i
$$ -->
<p>Expectation of a function $f(x)$ with respect to the categorical distribution is computed as</p>
<div>
$$
\mathrm{E}_{x\sim Cat(x;\theta)} f(x) = \sum_{i=1}^{K} \theta_i f(i)
$$
</div>
<h3 id="deriving-the-elbo">Deriving the ELBO</h3>
<p>Using these results above, we have</p>
<div>
\begin{align}
& \mathrm{E}_{q}\log Dir(\theta; \alpha) \\
& = \mathrm{E}_{q} \Big[\sum_{k=1}^{K} (\alpha_k - 1) \log \theta_k + \log \Gamma(\sum_{k=1}^{K}\alpha_k) - \sum_{k=1}^{K}\log\Gamma(\alpha_k)\Big] \\
& = \sum_{k=1}^{K}(\alpha_k - 1) \mathrm{E}_{\theta \sim Dir(\theta;\gamma)} \log\theta_k + \log \Gamma(\sum_{k=1}^{K}\alpha_k) - \sum_{k=1}^{K}\log\Gamma(\alpha_k) \\
& = \sum_{k=1}^{K}(\alpha_k - 1) (\Psi(\gamma_k) - \Psi(\sum_{j=1}^{K}\gamma_j)) + \log \Gamma(\sum_{k=1}^{K}\alpha_k) - \sum_{k=1}^{K}\log\Gamma(\alpha_k) \tag{3} \\
&\\
& \mathrm{E}_{q} \log Cat(z_n; \theta) \\
& = E_{z_n \sim Cat(z_n;\phi_n), \theta \sim Dir(\theta;\gamma)} \log Cat(z_n;\theta) \\
& = E_{z_n \sim Cat(z_n;\phi_n), \theta \sim Dir(\theta;\gamma)} \sum_{j=1}^{K}[z_n=j]\log \theta_j \\
& = \sum_{i=1}^{K} \phi_{ni} \mathrm{E}_{\theta \sim Dir(\theta; \gamma)} \sum_{j=1}^{K}[i=j]\log \theta_j \\
& = \sum_{i=1}^{K} \phi_{ni} \mathrm{E}_{\theta \sim Dir(\theta; \gamma)} \log \theta_i \\
& = \sum_{i=1}^{K} \phi_{ni}(\Psi(\gamma_i) - \Psi(\sum_{j=1}^{K}\gamma_j)) \tag{4} \\
&\\
& \mathrm{E}_{q} \log Cat(w_n;z_n, \beta) \\
& = \mathrm{E}_{z_n \sim Cat(z_n;\phi_n)} \log Cat(w_n;\beta_{z_n}) \\
& = \mathrm{E}_{z_n \sim Cat(z_n;\phi_n)} \sum_{j=1}^{V} [w_n=j] \log \beta_{z_n j} \\
&\quad \textrm{(assumming that $w_n$ represents the index of word in the vocabulary)} \\
& = \sum_{i=1}^{K} \phi_{ni} \sum_{j=1}^{V} [w_n=j] \log \beta_{ij}\\
& = \sum_{i=1}^{K} \sum_{j=1}^{V} \phi_{ni} [w_n=j] \log \beta_{ij} \tag{5} \\
&\\
& \mathrm{E}_{q} \log Dir(\theta;\gamma) \\
& = \mathrm{E}_{q} \Big[\sum_{k=1}^{K} (\gamma_k - 1) \log \theta_k + \log \Gamma(\sum_{k=1}^{K}\gamma_k) - \sum_{k=1}^{K}\log\Gamma(\gamma_k)\Big] \\
& = \sum_{k=1}^{K}(\gamma_k - 1) \mathrm{E}_{\theta \sim Dir(\theta;\gamma)} \log\theta_k + \log \Gamma(\sum_{k=1}^{K}\gamma_k) - \sum_{k=1}^{K}\log\Gamma(\alpha_k) \\
& = \sum_{k=1}^{K}(\gamma_k - 1) (\Psi(\gamma_k) - \Psi(\sum_{j=1}^{K}\gamma_j)) + \log \Gamma(\sum_{k=1}^{K}\gamma_k) - \sum_{k=1}^{K}\log\Gamma(\gamma_k) \tag{6} \\
&\\
& \mathrm{E}_{q} \log q(z_n; \phi_n) \\
& = \mathrm{E}_{z_n \sim Cat(z_n;\phi_n)} \log Cat(z_n;\phi_n) \\
& = \mathrm{E}_{z_n \sim Cat(z_n;\phi_n)} \sum_{j=1}^{K}[z_n = j]\log \phi_{nj} \\
& = \sum_{i=1}^{K} \phi_{ni} \sum_{j=1}^{K} [i=j] \log \phi_{nj} \\
& = \sum_{i=1}^{K} \phi_{ni} \log \phi_{ni} \tag{7}
\end{align}
</div>
<p>Substituting $(3), (4), (5), (6), (7)$ into $(1)$, the ELBO becomes</p>
<div>
\begin{align}
L&(\gamma, \phi;\alpha, \beta) \\
= &\sum_{k=1}^{K}(\alpha_k - 1) (\Psi(\gamma_k) - \Psi(\sum_{j=1}^{K}\gamma_j)) + \log \Gamma(\sum_{k=1}^{K}\alpha_k) - \sum_{k=1}^{K}\log\Gamma(\alpha_k) \\
& + \sum_{n=1}^{N} \sum_{i=1}^{K} \phi_{ni}(\Psi(\gamma_i) - \Psi(\sum_{j=1}^{K}\gamma_j)) \\
& + \sum_{n=1}^{N} \sum_{i=1}^{K} \sum_{j=1}^{V} \phi_{ni} [w_n = j]\log \beta_{ij}\\
& - \sum_{k=1}^{K}(\gamma_k - 1) (\Psi(\gamma_k) - \Psi(\sum_{j=1}^{K}\gamma_j)) - \log \Gamma(\sum_{k=1}^{K}\gamma_k) + \sum_{k=1}^{K}\log\Gamma(\gamma_k) \\
& - \sum_{n=1}^{N} \sum_{i=1}^{K} \phi_{ni} \log \phi_{ni} \tag{8}
\end{align}
</div>
which is now much easier to deal with.
<h2 id="preparing-data">Preparing data</h2>
<p>Now we dive into the code. For illustration purpose, we use a public dataset from Kaggle. The dataset contains news headlines crawled from ABC News. Here is some code to load the data</p>
<pre><code class="language-python">news_data_path = &quot;abcnews-date-text.csv&quot;

gdown.download(&quot;https://drive.google.com/uc?id=1BGaMi0XURByE0WM4omDwskoq83WnTXyx&quot;,
               news_data_path,
               quiet=False)

data_df = pd.read_csv(news_data_path, error_bad_lines=False);
data_df.head()
</code></pre>
<p>A small piece of the data will look like this</p>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}

td, th, tr {
    border: 1px solid #ddd;
}
</code></pre>
<p></style></p>
<table border="0" class="dataframe" style="display:table;">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>publish_date</th>
      <th>headline_text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>20030219</td>
      <td>aba decides against community broadcasting lic...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20030219</td>
      <td>act fire witnesses must be aware of defamation</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20030219</td>
      <td>a g calls for infrastructure protection summit</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20030219</td>
      <td>air nz staff in aust strike for pay rise</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20030219</td>
      <td>air nz strike to affect australian travellers</td>
    </tr>
  </tbody>
</table>
</div>
<h2 id="preprocessing-data">Preprocessing data</h2>
<p>There is a total of $1186018$ headlines in the original dataset but for a quick experiment, we extract the very first $10000$ headlines only</p>
<pre><code class="language-python">data = data_df[&quot;headline_text&quot;][:10000]
</code></pre>
<p>Then, we need to do some preprocessing stuff</p>
<ul>
<li>Remove the stop words using <code>stopwords</code> from <code>nltk</code> package</li>
<li>Build the vocabulary with <code>word2idx</code> and <code>idx2word</code></li>
<li>Create the <code>corpus</code> containing all documents</li>
</ul>
<pre><code class="language-python">corpus = []
word2idx = {}
idx2word = {}

for line in data:
    doc = [w for w in line.split(' ') if w not in stopwords.words()]
    for word in doc:
        if word not in word2idx:
            word2idx[word] = len(word2idx)
            idx2word[len(idx2word)] = word

    corpus.append(doc)
</code></pre>
<h2 id="global-configuration">Global configuration</h2>
<p>Next, we set up some global configuration before implementing LDA model.</p>
<pre><code class="language-python">max_doc_length = 0
for doc in corpus:
    if max_doc_length &lt; len(doc):
        max_doc_length = len(doc)

class Config:
    corpus = corpus
    word2idx = word2idx
    idx2word = idx2word
    num_vocabs = len(word2idx) # V
    max_doc_length = max_doc_length # N
    va_threshold = 1e-6 # threshold for variational infrence
    em_threshold = 1e-4 # threshold for variational EM
</code></pre>
<h2 id="lda-model-definition">LDA model definition</h2>
<p>We then define an <code>LDA</code> class to handle the main logic of Variational EM</p>
<pre><code class="language-python">class LDA(object):
    def __init__(self,
                 corpus,
                 num_topics,
                 num_words,
                 num_vocabs,
                 word2idx,
                 idx2word):

        self.corpus = corpus # collection of documents
        self.K = num_topics # number of topics in total
        self.V = num_vocabs # number of vocabulary

        self.word2idx = word2idx
        self.idx2word = idx2word

        # model parameters
        self.alpha = None
        self.beta = None

        # sufficient statistics
        self.beta_ss = None
</code></pre>
<h2 id="compute-the-log-likelihood">Compute the log-likelihood</h2>
<p>Evaluating $(8)$ requires the computation of Gamma and Digamma functions. Fortunately, we can make use of the <code>scipy</code> package to handle the heavy work.</p>
<pre><code class="language-python">from scipy.special import digamma, loggamma
</code></pre>
<p>We can then implement the <code>log_likelihood</code> function for current document and variational parameters $\gamma$, $\phi$; given model parameters $\alpha, \beta$</p>
<pre><code class="language-python">def log_likelihood(doc,
                   gamma,
                   phi):
    &quot;&quot;&quot;
    Compute the (approximate) log-likelihood
    &quot;&quot;&quot;
    # (K,)
    digamma_derivative = digamma(gamma) - digamma(np.sum(gamma))

    l1 = loggamma(np.sum(self.alpha)) - np.sum(loggamma(self.alpha)) \
        + np.sum((self.alpha - 1) * digamma_derivative) \
        - loggamma(np.sum(gamma)) + np.sum(loggamma(gamma)) \
        - np.sum((gamma - 1) * digamma_derivative)

    l2 = 0
    for i in range(self.K):
        for n in range(len(doc)):
            if phi[n, i] &gt; 0:
                l2 += phi[n, i] * (digamma_derivative[i] \
                    + np.log(self.beta[i, self.word2idx[doc[n]]]) \
                    - np.log(phi[n, i]))

    return l1 + l2
</code></pre>
<h2 id="variational-inference">Variational Inference</h2>
<p>The goal of variational inference is to find the optimal $\phi^*, \gamma^*$ for the mean-field distribution $q(z, \theta; \gamma, \phi)$. By taking derivatives of $(8)$ with respect to $\phi, \gamma$ and set it to zero, we obtain coordinate updates
$$
\phi_{ni} \propto \beta_{iv} \exp(\Psi(\gamma_i) - \Psi(\sum_{j=1}^{K}\gamma_j))
$$</p>
<p>$$
\gamma_i = \alpha_i + \sum_{n=1}^{N} \phi_{ni}
$$</p>
<p>where $v$ denotes the unique index of the word $w_n$ in the vocabulary. For more detailed derivation of these updates, we refer to Appendix A (section 3.1, 3.2) of the original LDA paper. Then, we can implement the coordinate ascent algorithm for variational inference as follows</p>
<pre><code class="language-python">def variational_inference(self, doc):
    &quot;&quot;&quot;
    Do the variational inference for each document
    &quot;&quot;&quot;

    N = len(doc)
    # init variational parameters
    # (N, K)
    phi = np.full(N, self.K), 1.0 / self.K)
    # (K,)
    gamma = self.alpha + N * 1.0 / self.K

    old_likelihood = -math.inf

    # coordinate ascent
    while True:
        # update phi
        for n in range(N):
            for i in range(self.K):
                phi[n, i] = self.beta[i, self.word2idx[doc[n]]] \
                * np.exp(digamma(gamma[i]))

        # normalize phi   
        phi = phi / np.sum(phi, axis=1, keepdims=True)

        # update gamma
        gamma = self.alpha + np.sum(phi, axis=0)

        likelihood = self.log_likelihood(doc, gamma, phi)

        converged = (old_likelihood - likelihood) / likelihood

        old_likelihood = likelihood

        if converged &lt; cfg.va_threshold:
            break

    return phi, gamma, likelihood
</code></pre>
<h2 id="variational-em">Variational EM</h2>
<p>Recall that the Variational EM algorithm consisting of</p>
<ul>
<li>Initialize parameters $\alpha^{(0)}, \beta^{(0)}$</li>
<li>For each loop $t$ start from $0$
<ul>
<li><strong>E step</strong>: For each document $d$, obtain the optimal $\gamma^{(d)}, \phi^{(d)}$ of the variational distribution $q(z, \theta; \gamma, \phi) = q(\theta;\gamma)\prod_{n=1}^{N}q(z_n;\phi_n)$</li>
<li><strong>M step</strong>: Maximize the expected log-likelihood (up to some constant)
  <div>$$
  \mathop{max}_{{\alpha^{(t+1)}, \beta^{(t+1)}}} \sum_{d=1}^{M} \mathrm{E}_{z, \theta \sim q(z, \theta; \gamma^{(d)}, \phi^{(d)})} {\log p(w, z, \theta ;{{\alpha^{(t+1)}, \beta^{(t+1)}}}})
  $$</div>
</li>
<li>If the convergence standard is satisfied, stop</li>
</ul>
</li>
</ul>
<p>Hence, the Variational EM algorithm can be implemented as function <code>variational_em</code> below</p>
<pre><code class="language-python">def variational_em(self):
    &quot;&quot;&quot;
    Fit LDA model using variational EM
    &quot;&quot;&quot;

    self.init_param()

    old_llhood = -math.inf

    ite = 0

    while True:
        ite += 1
        llhood = self.variational_e_step(self.corpus)
        self.m_step()

        converged = (old_llhood - llhood) / llhood
        old_llhood = llhood

        print(&quot;STEP EM: {} - Likelihood: {} - Converged rate: {}&quot;.\
            format(ite, llhood, converged))

        if converged &lt; cfg.em_threshold:
            break
</code></pre>
<p>The function <code>init_param</code> is to initialize parameter $\alpha, \beta$</p>
<pre><code class="language-python"> def init_param(self):
    &quot;&quot;&quot;
    Init parameters
    &quot;&quot;&quot;
    self.alpha = np.full(self.K, 1.0)
    self.beta = np.random.randint(1, 50, (self.K, self.V))
    self.beta = self.beta / np.sum(self.beta, axis=1, keepdims=True)
</code></pre>
<p>Then, two functions <code>variational_e_step</code> and <code>m_step</code> are corresponding to <code>E-step</code> and <code>M-step</code> of the algorithm, respectively.</p>
<h3 id="e-step">E Step</h3>
<p>In the <code>E-step</code>, we perform variational inference for each document $d$ to obtain $\phi^{(d)}, \gamma^{(d)}$</p>
<pre><code class="language-python">def variational_e_step(self, corpus):
    &quot;&quot;&quot;
    Approximate the posterior distribution

    : corpus - list of documents

    &quot;&quot;&quot;

    total_likelihood = 0

    self.beta_ss = np.zeros((self.K, self.V)) + 1e-20

    for i, doc in enumerate(corpus):
        phi, gamma, doc_likelihood = self.variational_inference(doc)

        # add to total likelihood
        total_likelihood += doc_likelihood

        # update statistics
        for n in range(len(doc)):
            for k in range(self.K):
                self.beta_ss[k, self.word2idx[doc[n]]] += phi[n, k]

    return total_likelihood
</code></pre>
<h3 id="m-step">M Step</h3>
<p>In <code>M-step</code>, we obtain optimal $\alpha, \beta$. Though, the optimal update for $\alpha$ is kind of complex (Appendix A, section 4.2). Thus, for a simple illustration, we consider <code>alpha</code> as fixed in the scope of this blog. Setting the derivate of the ELBO $(8)$ with respect to $\beta$ to zero, we yield</p>
<p>$$
\beta_{ij} \propto \sum_{d=1}^{M} \sum_{n=1}^{N} \phi_{ni}^{(d)} w_{n}^{j} \quad \textrm{(Appendix A, section 4.1)}
$$</p>
<p>For coding convenience, we implement these updates right in the function <code>variational_e_step</code> and store these unnormalized results in the variable <code>self.beta_ss</code>. Hence, in the <code>M-step</code>, we just normalize and assign it to <code>self.beta</code></p>
<pre><code class="language-python">def m_step(self):
    &quot;&quot;&quot;
    Maximum likelihood estimation
    &quot;&quot;&quot;

    # alpha is considered fixed known constant, hence skip here
    # self.alpha

    # (K, V)
    self.beta = self.beta_ss / np.sum(self.beta_ss, axis=1, keepdims=True)

</code></pre>
<h2 id="training">Training</h2>
<p>Now everything is setup. We then run the following code for training LDA</p>
<pre><code class="language-python">model = LDA(corpus=cfg.corpus,
            num_topics=10,
            num_words=cfg.max_doc_length,
            num_vocabs=cfg.num_vocabs,
            word2idx=cfg.word2idx,
            idx2word=cfg.idx2word)

model.variational_em()
</code></pre>
<p>The output will be like</p>
<pre><code>STEP EM: 1 - Likelihood: -357373.4968604174 - Converged rate: 1.798192951590305
STEP EM: 2 - Likelihood: -303489.3001228882 - Converged rate: 0.1775489175918577
STEP EM: 3 - Likelihood: -299860.97107707005 - Converged rate: 0.012100037670076039
STEP EM: 4 - Likelihood: -294189.9329730006 - Converged rate: 0.019276791856062188
STEP EM: 5 - Likelihood: -287787.73198189674 - Converged rate: 0.02224626097510863
STEP EM: 6 - Likelihood: -282157.87102739484 - Converged rate: 0.0199528757925569
STEP EM: 7 - Likelihood: -277893.16594093136 - Converged rate: 0.015346563388931922
...........................................
STEP EM: 32 - Likelihood: -264753.70609608945 - Converged rate: 0.00017462683995490507
STEP EM: 33 - Likelihood: -264713.55673188687 - Converged rate: 0.00015167097861647994
STEP EM: 34 - Likelihood: -264676.9793795508 - Converged rate: 0.0001381961983313688
STEP EM: 35 - Likelihood: -264643.03067223117 - Converged rate: 0.00012828113112741676
STEP EM: 36 - Likelihood: -264611.45086573914 - Converged rate: 0.000119344066134337
STEP EM: 37 - Likelihood: -264582.51780543657 - Converged rate: 0.0001093536358432039
STEP EM: 38 - Likelihood: -264556.4361364331 - Converged rate: 9.85864089506385e-05
</code></pre>
<h2 id="result">Result</h2>
<p>After training, we can extract the top words of the 10 &ldquo;abstract&rdquo; topics.</p>
<pre><code class="language-python">topk = 10

indices = np.argpartition(model.beta, -topk, axis=1)[:, -topk:]

topic_top_words_dict = {}
for k in range(model.K):
    topic_top_words_dict[&quot;Topic {}&quot;.format(k + 1)] = \
        [model.idx2word[idx] for idx in indices[k]]

topic_df = pd.DataFrame(topic_top_words_dict)
topic_df
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="0" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Topic 1</th>
      <th>Topic 2</th>
      <th>Topic 3</th>
      <th>Topic 4</th>
      <th>Topic 5</th>
      <th>Topic 6</th>
      <th>Topic 7</th>
      <th>Topic 8</th>
      <th>Topic 9</th>
      <th>Topic 10</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>air</td>
      <td>murder</td>
      <td>gets</td>
      <td>continue</td>
      <td>denies</td>
      <td>tas</td>
      <td>community</td>
      <td>high</td>
      <td>act</td>
      <td>lead</td>
    </tr>
    <tr>
      <th>1</th>
      <td>takes</td>
      <td>probe</td>
      <td>season</td>
      <td>protesters</td>
      <td>farmers</td>
      <td>south</td>
      <td>public</td>
      <td>ban</td>
      <td>three</td>
      <td>australian</td>
    </tr>
    <tr>
      <th>2</th>
      <td>baghdad</td>
      <td>charged</td>
      <td>trial</td>
      <td>water</td>
      <td>vic</td>
      <td>final</td>
      <td>go</td>
      <td>work</td>
      <td>forces</td>
      <td>union</td>
    </tr>
    <tr>
      <th>3</th>
      <td>election</td>
      <td>aust</td>
      <td>saddam</td>
      <td>found</td>
      <td>korea</td>
      <td>clash</td>
      <td>water</td>
      <td>continues</td>
      <td>claims</td>
      <td>four</td>
    </tr>
    <tr>
      <th>4</th>
      <td>first</td>
      <td>pm</td>
      <td>home</td>
      <td>back</td>
      <td>boost</td>
      <td>get</td>
      <td>missing</td>
      <td>wins</td>
      <td>dead</td>
      <td>minister</td>
    </tr>
    <tr>
      <th>5</th>
      <td>coast</td>
      <td>crash</td>
      <td>top</td>
      <td>plan</td>
      <td>urged</td>
      <td>world</td>
      <td>new</td>
      <td>set</td>
      <td>council</td>
      <td>mp</td>
    </tr>
    <tr>
      <th>6</th>
      <td>oil</td>
      <td>may</td>
      <td>win</td>
      <td>anti</td>
      <td>govt</td>
      <td>call</td>
      <td>killed</td>
      <td>calls</td>
      <td>support</td>
      <td>qld</td>
    </tr>
    <tr>
      <th>7</th>
      <td>coalition</td>
      <td>iraqi</td>
      <td>still</td>
      <td>death</td>
      <td>iraq</td>
      <td>cup</td>
      <td>fire</td>
      <td>funds</td>
      <td>us</td>
      <td>british</td>
    </tr>
    <tr>
      <th>8</th>
      <td>howard</td>
      <td>court</td>
      <td>two</td>
      <td>sars</td>
      <td>north</td>
      <td>australia</td>
      <td>rain</td>
      <td>security</td>
      <td>troops</td>
      <td>wa</td>
    </tr>
    <tr>
      <th>9</th>
      <td>nsw</td>
      <td>police</td>
      <td>woman</td>
      <td>protest</td>
      <td>health</td>
      <td>report</td>
      <td>hospital</td>
      <td>drought</td>
      <td>says</td>
      <td>group</td>
    </tr>
  </tbody>
</table>
</div>
<p>Full code available 
<a href="https://colab.research.google.com/drive/15nqnmXiA3RnfiYPHDrRVMjJVTdszaGJ-?usp=sharing" target="_blank" rel="noopener">here</a>.</p>
<p><strong>References</strong></p>
<ol>
<li>Latent Dirichlet Allocation. David M. Blei, Andrew Y. Ng, Michael I. Jordan. 2003 (
<a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf" target="_blank" rel="noopener">pdf</a>)</li>
<li>Dataset (
<a href="https://www.kaggle.com/therohk/million-headlines" target="_blank" rel="noopener">link</a>)</li>
</ol>

          </div>
          





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/latent-dirichlet-allocation/">latent dirichlet allocation</a>
  
  <a class="badge badge-light" href="/tags/lda/">lda</a>
  
  <a class="badge badge-light" href="/tags/topic-modeling/">topic modeling</a>
  
  <a class="badge badge-light" href="/tags/python/">python</a>
  
  <a class="badge badge-light" href="/tags/variational-inference/">variational inference</a>
  
  <a class="badge badge-light" href="/tags/variational-em/">variational em</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://azraelzhor.github.io/post/lda-part-2/&amp;text=Variational%20Expectation%20Maximization%20for%20Latent%20Dirichlet%20Allocation%20-%20Part%202" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://azraelzhor.github.io/post/lda-part-2/&amp;t=Variational%20Expectation%20Maximization%20for%20Latent%20Dirichlet%20Allocation%20-%20Part%202" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Variational%20Expectation%20Maximization%20for%20Latent%20Dirichlet%20Allocation%20-%20Part%202&amp;body=https://azraelzhor.github.io/post/lda-part-2/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://azraelzhor.github.io/post/lda-part-2/&amp;title=Variational%20Expectation%20Maximization%20for%20Latent%20Dirichlet%20Allocation%20-%20Part%202" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Variational%20Expectation%20Maximization%20for%20Latent%20Dirichlet%20Allocation%20-%20Part%202%20https://azraelzhor.github.io/post/lda-part-2/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://azraelzhor.github.io/post/lda-part-2/&amp;title=Variational%20Expectation%20Maximization%20for%20Latent%20Dirichlet%20Allocation%20-%20Part%202" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      <img class="avatar mr-3 avatar-circle" src="https://s.gravatar.com/avatar/046350be6588e8b350c18eb14e96f5dc?s=200')" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://azraelzhor.github.io/">Thang Le</a></h5>
      <h6 class="card-subtitle">ML Enthusiast</h6>
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/joocxi" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://facebook.com/joocxi" target="_blank" rel="noopener">
        <i class="fab fa-facebook"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/azraelzhor" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>




<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "joocxi" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>




<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/lda-part-1/" rel="prev">Variational Expectation Maximization for Latent Dirichlet Allocation - Part 1</a>
  </div>
  
</div>

</div>



  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/lda-part-1/">Variational Expectation Maximization for Latent Dirichlet Allocation - Part 1</a></li>
      
      <li><a href="/talk/latent-dirichlet-allocation/">Latent Dirichlet Allocation</a></li>
      
    </ul>
  </div>
  


        </div>
      </article>

    </main>
  </div>
</div>


      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://joocxi.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.a8d7005002cb4a052fd6d721e83df9ba.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    joocxi@2020 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
